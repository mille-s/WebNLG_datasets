{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0BvThbbOWguw",
        "DKu-LjO5-Doe",
        "0wlGgCYRVs04",
        "eORSytr9VLej",
        "vEQ6kkJWnTgW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/WebNLG_datasets/blob/main/WebNLG_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Google Translate (first because otherwise we need to restart session)\n",
        "# https://pypi.org/project/googletrans/\n",
        "from IPython.display import clear_output\n",
        "! pip install googletrans==3.1.0a0\n",
        "clear_output()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "b-KpK8bRwn-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Shared packages and functions\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Package for parsing xml files (WebNLG 23 and Enhanced WebNLG)\n",
        "! pip install xmltodict\n",
        "\n",
        "# Install SPARQLWrapper for making queries to DBpedia/Wikidata\n",
        "! pip install SPARQLWrapper\n",
        "\n",
        "# datasets is for loading datasets from HuggingFace (WebNLG 17, 18, 20)\n",
        "! pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "! pip install --upgrade gdown\n",
        "\n",
        "! pip install dicttoxml\n",
        "\n",
        "# Clone repos containing WebNLG processing modules and processed data\n",
        "! git clone 'https://github.com/mille-s/Mod-D2T.git'\n",
        "! git clone 'https://github.com/mille-s/M-FleNS_NLG-Pipeline.git'\n",
        "! git clone 'https://github.com/mille-s/UD_Converter.git'\n",
        "! git clone 'https://github.com/mille-s/DCU_TCD_FORGe_WebNLG23.git'\n",
        "# Delete locally to avoid confusion\n",
        "! rm '/content/UD_Converter/UD_Converter_release.ipynb'\n",
        "! rm '/content/Mod-D2T/Mod-D2T.ipynb'\n",
        "! rm '/content/M-FleNS_NLG-Pipeline/M-FleNS_NLG-Pipeline.ipynb'\n",
        "! rm '/content/DCU_TCD_FORGe_WebNLG23/DCU_TCD_FORGe_WebNLG23.ipynb'\n",
        "\n",
        "clear_output()\n",
        "\n",
        "def extractTripleElements(dataset, element):\n",
        "  \"\"\" Returns a list of unique subjects, objects or properties extracted from triple sets\"\"\"\n",
        "  n = ''\n",
        "  if element == 'subject':\n",
        "    n = 0\n",
        "  elif element == 'property':\n",
        "    n = 1\n",
        "  elif element == 'object':\n",
        "    n = 2\n",
        "  else:\n",
        "    print('Error, the second argument of extractTripleElements must be \"subject\", \"property\" or \"object\".')\n",
        "  element_list = []\n",
        "  for entry in dataset:\n",
        "    for input_triple in entry[0]:\n",
        "      element_name = input_triple.split(' | ')[n]\n",
        "      if element_name not in element_list:\n",
        "        element_list.append(element_name)\n",
        "  return(element_list)"
      ],
      "metadata": {
        "id": "VmOq8W6Qcb2G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions for wikidata and dbpedia queries\n",
        "# Function list\n",
        "\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import progressbar\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "bar = ''\n",
        "def createProgressBar(bar, max):\n",
        "  bar = progressbar.ProgressBar(max_value=max)\n",
        "  return(bar)\n",
        "\n",
        "def format_entity_dbp(entity):\n",
        "  \"\"\"\n",
        "  Used for the 2024 experiments\n",
        "  \"\"\"\n",
        "  # Add this line so all lines below have the same variable name on the right\n",
        "  clean_entity = entity\n",
        "  # Remove what is between parentheses; in the end better to keep and escape them\n",
        "  # clean_entity = clean_entity.split('_(',1)[0]\n",
        "  # clean_entity = clean_entity.split(' (',1)[0]\n",
        "  # Replace underscores by spaces (for wikidata)\n",
        "  # clean_entity = re.sub('_', ' ', clean_entity)\n",
        "  # Replace ampersands by 'and' (for dbpedia, seems to affect results from wikidata though)\n",
        "  # clean_entity = re.sub('&', 'and', clean_entity)\n",
        "  # Escape other reserved characters\n",
        "  clean_entity = re.sub('/', '\\/', clean_entity)\n",
        "  clean_entity = re.sub('\\.', '\\.', clean_entity)\n",
        "  clean_entity = re.sub('\\+', '\\+', clean_entity)\n",
        "  clean_entity = re.sub('\\,', '\\,', clean_entity)\n",
        "  clean_entity = re.sub('\\&', '\\&', clean_entity)\n",
        "  clean_entity = re.sub('\\-', '\\-', clean_entity)\n",
        "  clean_entity = re.sub('\\(', '\\(', clean_entity)\n",
        "  clean_entity = re.sub('\\)', '\\)', clean_entity)\n",
        "  # Remove quotes, semi-colons and other things which are usually errors or hacks\n",
        "  clean_entity = re.sub('\"', '', clean_entity)\n",
        "  clean_entity = re.sub(';', '', clean_entity)\n",
        "  clean_entity = re.sub('~', '', clean_entity)\n",
        "  clean_entity = re.sub('<', '', clean_entity)\n",
        "  clean_entity = re.sub('>', '', clean_entity)\n",
        "  # Other\n",
        "  # I checked, it works like this...\n",
        "  clean_entity = re.sub(\"'\", \"\\\\'\", clean_entity)\n",
        "  return clean_entity\n",
        "\n",
        "def format_entity_wkd(entity):\n",
        "  \"\"\"\n",
        "  Used for the GEM 2023-2024 data\n",
        "  \"\"\"\n",
        "  # Remove what is after commas and between parentheses\n",
        "  clean_entity = entity.split(',',1)[0].split('_(',1)[0]\n",
        "  # Replace underscores by spaces (for wikidata)\n",
        "  clean_entity = re.sub('_', ' ', clean_entity)\n",
        "  # Remove quotes\n",
        "  clean_entity = re.sub('\"', '', clean_entity)\n",
        "  return clean_entity\n",
        "\n",
        "def assign_classRegEx(entity):\n",
        "  classRegEx = ''\n",
        "  if re.search('gramPerCubicCentimetres', entity):\n",
        "    classRegEx = 'concentration_gPerCubCm'\n",
        "  if re.search('kilogramPerCubicMetres', entity):\n",
        "    classRegEx = 'concentration_kgPerCubM'\n",
        "  elif re.search('inhabitants per square kilometre', entity):\n",
        "    classRegEx = 'populationDensity'\n",
        "  elif re.search('[0-9\\.,]+.*square.*metre', entity):\n",
        "    classRegEx = 'area_measurement'\n",
        "  elif re.search('bombing', entity):\n",
        "    classRegEx = 'event'\n",
        "  elif re.search('[Uu]niversity', entity):\n",
        "    classRegEx = 'university'\n",
        "  elif re.search('Dodge', entity):\n",
        "    classRegEx = 'car'\n",
        "  elif re.search('^\"*[0-9]{4}-[0-9]{2}-[0-9]{2}\"*$', entity):\n",
        "    classRegEx = 'date'\n",
        "  elif re.search('^\"*[0-9]+\\s*-*(January|Jan|February|Feb|March|Mar|April|Apr|May|June|Jun|July|Jul|August|Aug|September|Sept|October|Oct|November|Nov|December|Dec)\\s*-*[0-9]+\"*$', entity):\n",
        "    classRegEx = 'date'\n",
        "  elif re.search('^\"*(January|February|March|April|May|June|July|August|September|October|November|December)\\s*-*[0-9]{4}\"*$', entity):\n",
        "    classRegEx = 'month'\n",
        "  elif re.search('^\"*(January|February|March|April|May|June|July|August|September|October|November|December)\"*$', entity):\n",
        "    classRegEx = 'month'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*(litres|cubic)', entity):\n",
        "    classRegEx = 'volume_measurement'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*m\"*$', entity):\n",
        "    classRegEx = 'distance_meters'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*in\"*$', entity):\n",
        "    classRegEx = 'distance_inches'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*yd\"*$', entity):\n",
        "    classRegEx = 'distance_yards'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*ft\"*$', entity):\n",
        "    classRegEx = 'distance_feet'\n",
        "  elif re.search('[0-9\\.,]+.*millimetres', entity):\n",
        "    classRegEx = 'distance_millimetres'\n",
        "  elif re.search('[0-9\\.,]+.*centimetres', entity):\n",
        "    classRegEx = 'distance_centimetres'\n",
        "  elif re.search('[0-9\\.,]+.*metres', entity):\n",
        "    classRegEx = 'distance_metres'\n",
        "  elif re.search('[0-9\\.,]+.*inches', entity):\n",
        "    classRegEx = 'distance_inches'\n",
        "  elif re.search('[0-9\\.,]+.*yards', entity):\n",
        "    classRegEx = 'distance_yards'\n",
        "  elif re.search('[0-9\\.,]+.*feet', entity):\n",
        "    classRegEx = 'distance_feet'\n",
        "  elif re.search('[0-9\\.,]+.*seconds', entity):\n",
        "    classRegEx = 'duration_seconds'\n",
        "  elif re.search('[0-9\\.,]+.*minutes', entity):\n",
        "    classRegEx = 'duration_minutes'\n",
        "  elif re.search('[0-9\\.,]+.*hours', entity):\n",
        "    classRegEx = 'duration_hours'\n",
        "  elif re.search('[0-9\\.,]+.*days', entity):\n",
        "    classRegEx = 'duration_days'\n",
        "  elif re.search('[0-9\\.,]+.*weeks', entity):\n",
        "    classRegEx = 'duration_weeks'\n",
        "  elif re.search('[0-9\\.,]+.*months', entity):\n",
        "    classRegEx = 'duration_months'\n",
        "  elif re.search('[0-9\\.,]+.*years', entity):\n",
        "    classRegEx = 'duration_years'\n",
        "  elif re.search('[0-9\\.,]+.* (engine|horsepower)', entity):\n",
        "    classRegEx = 'engine'\n",
        "  elif re.search('[0-9\\.,]+.*euros', entity):\n",
        "    classRegEx = 'moneyQuantity_euros'\n",
        "  elif re.search('[0-9\\.,]+.*dollars', entity):\n",
        "    classRegEx = 'moneyQuantity_dollars'\n",
        "  elif re.search('[0-9\\.,]+.*kilometrePerSeconds', entity):\n",
        "    classRegEx = 'speed_kmPerSec'\n",
        "  elif re.search('[0-9\\.,]+.*degreeCelsius', entity):\n",
        "    classRegEx = 'temperature_celsius'\n",
        "  elif re.search('[0-9\\.,]+.*kelvins', entity):\n",
        "    classRegEx = 'temperature_kelvin'\n",
        "  elif re.search('[0-9\\.,]+-speed', entity):\n",
        "    classRegEx = 'transmission'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*(\\sg|grams)', entity):\n",
        "    classRegEx = 'weight_grams'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*\\skg', entity):\n",
        "    classRegEx = 'weight_kilograms'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*tonnes', entity):\n",
        "    classRegEx = 'weight_tonnes'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*pounds', entity):\n",
        "    classRegEx = 'weight_pounds'\n",
        "  elif re.search('^\"*[0-9]+/[0-9]+\"*$', entity):\n",
        "    classRegEx = 'fraction'\n",
        "  elif re.search('^\"*[0-9a-zA-Z]+/[0-9a-zA-Z\\s\\']+\"*$', entity):\n",
        "    classRegEx = 'runwayName'\n",
        "  elif re.search('^\"*[0-9]{4}[-–][0-9]{4}\"*$', entity):\n",
        "    classRegEx = 'issnNumber'\n",
        "  elif re.search('^\"*[0-9]+[-–][0-9]+[-–][0-9]+[-–][0-9]+[-–]*[0-9]*\"*$', entity):\n",
        "    classRegEx = 'isbnNumber'\n",
        "  elif re.search('^\"*[0-9]+[-–][0-9]+[-–]*[0-9]*[-–]*[0-9]*[-–]*[0-9]*\"*$', entity):\n",
        "    classRegEx = 'unknownIdentifier'\n",
        "  elif re.search('^\"*[0-9]{2} [a-zA-Z]+\"*$', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('^\"*[0-9]{3} [a-zA-Z]+\"*$', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('^\"*[0-9]+_[a-zA-Z]+\"*$', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('_FC_', entity):\n",
        "    classRegEx = 'footballClub'\n",
        "  elif re.search('(season|EPSTH|league|League|Liga|Season|Bundesliga|Eredivisie|Football_Conference|Lega_Pro|Regionalliga|Serie_A|Serie_B|Topklasse|Campeonato)', entity):\n",
        "    classRegEx = 'sportsSeason'\n",
        "  elif re.search('[Mm]onument', entity):\n",
        "    classRegEx = 'monument'\n",
        "  elif re.search('^\"*[0-9]{4} [a-zA-Z]+', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('^\"*[0-9-]+[stndr]*[\\s\\-_][^:]*[\\(\\)a-zA-Z\\']+\"*$', entity):\n",
        "    if not re.search('JD2457600', entity):\n",
        "      classRegEx = 'address'\n",
        "    else:\n",
        "      classRegEx = 'date_epoch'\n",
        "  elif re.search('^\"*[\\+-]*[0-9\\.,]+\"*$', entity):\n",
        "    classRegEx = 'unknownQuantity'\n",
        "  elif re.search('[0-9\\.,]+, [0-9\\.,]+', entity):\n",
        "    classRegEx = 'unknownQuantity_multiple'\n",
        "\n",
        "  return(classRegEx)\n",
        "\n",
        "# Code below adapted from ChatGPT\n",
        "def get_wikidata_id(entity_label):\n",
        "  # Define the Wikidata API endpoint\n",
        "  wikidata_api_url = \"https://www.wikidata.org/w/api.php\"\n",
        "\n",
        "  # Set the parameters for the API request\n",
        "  params = {\n",
        "    \"action\": \"wbsearchentities\",\n",
        "    \"format\": \"json\",\n",
        "    \"language\": \"en\",  # You can change the language if needed\n",
        "    \"search\": entity_label,\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    # Send a GET request to the Wikidata API\n",
        "    response = requests.get(wikidata_api_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    # Parse the JSON response\n",
        "    data = response.json()\n",
        "    # Check if any entities were found\n",
        "    if \"search\" in data and data[\"search\"]:\n",
        "      # Get the first entity (assuming it's the most relevant)\n",
        "      entity_id = data[\"search\"][0][\"id\"]\n",
        "      return entity_id\n",
        "    return None  # Entity not found\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(\"Error connecting to the Wikidata API:\", e)\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "# entity_label = '23 g'\n",
        "# wikidata_id = get_wikidata_id(entity_label)\n",
        "# if wikidata_id:\n",
        "#   print(f\"The Q-ID for {entity_label} is {wikidata_id}.\")\n",
        "# else:\n",
        "#   print(f\"No entity found for {entity_label}.\")\n",
        "# print(assign_classRegEx(entity_label))\n",
        "\n",
        "def get_wikidata_id_bulk(rows, list_entities, bar):\n",
        "  bar = createProgressBar(bar, len(list_entities)-1)\n",
        "  for count, entity in enumerate(list_entities):\n",
        "    bar.update(count)\n",
        "    row = []\n",
        "    clean_entity = ''\n",
        "    if entity == 'School of Business and Social Sciences at the Aarhus University':\n",
        "      clean_entity = 'Aarhus School of Business'\n",
        "    else:\n",
        "      clean_entity = format_entity_wkd(entity)\n",
        "    wikidata_id = get_wikidata_id(clean_entity)\n",
        "    # wikidata_id = None\n",
        "    if wikidata_id:\n",
        "      # print(f\"The Q-ID for {entity} is {wikidata_id}.\")\n",
        "      row.append(wikidata_id)\n",
        "      row.append(clean_entity)\n",
        "      row.append(entity)\n",
        "      row.append(assign_classRegEx(entity))\n",
        "    else:\n",
        "      # print(f\"No entity found for {entity}.\")\n",
        "      row.append('???')\n",
        "      row.append(clean_entity)\n",
        "      row.append(entity)\n",
        "      row.append(assign_classRegEx(entity))\n",
        "    rows.append(row)\n",
        "\n",
        "# ChatGPT prompt: Please write some Python code to get the value of an named entity's \"gold:hypernym\" property according to DBpedia\n",
        "def get_dbpedia_hypernym(entity_name):\n",
        "  entity_name = format_entity_dbp(entity_name)\n",
        "  # For DBpedia specifically, we need to replace spaces by underscores in entity names to avoid query errors\n",
        "  entity_name = ('_').join(entity_name.split(' '))\n",
        "\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT ?hypernym\n",
        "  WHERE {{\n",
        "    dbr:{entity_name} gold:hypernym ?hypernym\n",
        "  }}\n",
        "    \"\"\"\n",
        "\n",
        "  # Set the query and response format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Extract and return the hypernym value\n",
        "  if 'results' in results and 'bindings' in results['results']:\n",
        "    bindings = results['results']['bindings']\n",
        "    # print(bindings)\n",
        "    # Return the first value only\n",
        "    if bindings:\n",
        "      if re.search('/', bindings[0]['hypernym']['value']):\n",
        "        return bindings[0]['hypernym']['value'].rsplit('/',1)[1]\n",
        "      else:\n",
        "        return bindings[0]['hypernym']['value']\n",
        "\n",
        "    return None\n",
        "\n",
        "# ChatGPT prompt: Please write some Python code to get the value of an named entity's \"gold:hypernym\" property according to Wikidata\n",
        "def get_wikidata_hypernym(entity_ID):\n",
        "  # Define the Wikidata Query Service endpoint URL\n",
        "  wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT ?hypernymLabel\n",
        "  WHERE {{\n",
        "    wd:{entity_ID} wdt:P31 ?hypernym.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set up the request headers\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept': 'application/json'\n",
        "  }\n",
        "\n",
        "  # Set up the request parameters\n",
        "  params = {\n",
        "    'query': query,\n",
        "    'format': 'json'\n",
        "  }\n",
        "\n",
        "  # Make the API request\n",
        "  response = requests.get(wikidata_endpoint, headers=headers, params=params)\n",
        "\n",
        "  # Parse the JSON response\n",
        "  data = response.json()\n",
        "\n",
        "  # Extract and return the hypernym value\n",
        "  if 'results' in data and 'bindings' in data['results']:\n",
        "    bindings = data['results']['bindings']\n",
        "    if bindings:\n",
        "      return bindings[0]['hypernymLabel']['value']\n",
        "\n",
        "  return None\n",
        "\n",
        "def get_Wikidata_id_property(dbpedia_prop):\n",
        "  # Set up the SPARQL endpoint\n",
        "\n",
        "  wikidata_ids = []\n",
        "\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to get the Wikidata ID for a DBpedia property\n",
        "  query = f\"\"\"\n",
        "  SELECT ?wikidataProperty\n",
        "  WHERE {{\n",
        "    <{dbpedia_prop}> owl:equivalentProperty ?wikidataProperty .\n",
        "    FILTER(STRSTARTS(STR(?wikidataProperty), \"http://www.wikidata.org/entity/\"))\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "    # Execute the query\n",
        "    results = sparql.query().convert()\n",
        "\n",
        "    # Extract and print the Wikidata property ID\n",
        "    wikidata_properties = [result[\"wikidataProperty\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
        "    # print(wikidata_properties)\n",
        "    for prop in wikidata_properties:\n",
        "      # Extract the Wikidata ID (e.g., \"P569\") from the full URL\n",
        "      wikidata_id = prop.split('/')[-1]\n",
        "      if wikidata_id not in wikidata_ids:\n",
        "        wikidata_ids.append(wikidata_id)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return(wikidata_ids)"
      ],
      "metadata": {
        "id": "GoIxUIfCpvqz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original WebNLG"
      ],
      "metadata": {
        "id": "l9e6zzYSZkur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNP-s_7E8Whx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install and download\n",
        "# Download training data of WebNLG 23\n",
        "!gdown 110L41fyQpxDhgzUsraRNMqwo3AkvNgu7\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load datasets\n",
        "\n",
        "import xmltodict\n",
        "\n",
        "# Path to downloaded xml\n",
        "# path_file = '/content/ga_train.xml'\n",
        "# xml_file = open(path_file, 'r').read()\n",
        "# dataset_23_dict = xmltodict.parse(xml_file)\n",
        "\n",
        "# Load existing datasets from HuggingFace\n",
        "webnlg_2017 = load_dataset('web_nlg', 'webnlg_challenge_2017', trust_remote_code=True)\n",
        "webnlg_enriched = load_dataset('enriched_web_nlg', 'en',trust_remote_code=True)\n",
        "webnlg_2020 = load_dataset('web_nlg', 'release_v3.0_en', trust_remote_code=True)\n",
        "# webnlg_2020 = load_dataset('GEM/web_nlg', 'en')\n",
        "\n",
        "def load_webnlg(webnlg, split, data_origin):\n",
        "  \"Gets the mtriples and the verbalisations\"\n",
        "  dataset = []\n",
        "  for sample in webnlg[split]:\n",
        "    # if sample['size'] == 1:\n",
        "    # Next line is to get additional annotations from enhanced webnlg\n",
        "    # dataset.append([sample['modified_triple_sets']['mtriple_set'][0], sample['lex']['text'], sample['lex']['sorted_triple_sets'], sample['lex']['template']])\n",
        "    if data_origin == 'modified':\n",
        "      dataset.append([sample['modified_triple_sets']['mtriple_set'][0], sample['lex']['text']])\n",
        "    elif data_origin == 'original':\n",
        "      dataset.append([sample['original_triple_sets']['otriple_set'][0], sample['lex']['text']])\n",
        "  return dataset\n",
        "\n",
        "def load_webnlg_cat(webnlg, split, data_origin):\n",
        "  \"Gets the mtriples and the verbalisations, and the category. Was created separately to not break anything that was using the data produced by the load_webnlg function\"\n",
        "  dataset = []\n",
        "  for sample in webnlg[split]:\n",
        "    # if sample['size'] == 1:\n",
        "    # Next line is to get additional annotations from enhanced webnlg\n",
        "    # dataset.append([sample['modified_triple_sets']['mtriple_set'][0], sample['lex']['text'], sample['lex']['sorted_triple_sets'], sample['lex']['template']])\n",
        "    if data_origin == 'modified':\n",
        "      dataset.append([sample['modified_triple_sets']['mtriple_set'][0], sample['lex']['text'], sample['category']])\n",
        "    elif data_origin == 'original':\n",
        "      dataset.append([sample['original_triple_sets']['otriple_set'][0], sample['lex']['text'], sample['category']])\n",
        "  return dataset\n",
        "\n",
        "# One data point is 2 lists: triples, english texts\n",
        "# 1 triple example: [['Arianespace | country | France'], ['Arianespace is located in France.']]\n",
        "dataset_17_train = load_webnlg(webnlg_2017, 'train', 'modified')\n",
        "dataset_18_train = load_webnlg(webnlg_enriched, 'train', 'modified')\n",
        "dataset_20_train = load_webnlg(webnlg_2020, 'train', 'modified')\n",
        "dataset_20_dev = load_webnlg(webnlg_2020, 'dev', 'modified')\n",
        "dataset_20_test = load_webnlg(webnlg_2020, 'test', 'modified')\n",
        "\n",
        "dataset_20_train_orig = load_webnlg(webnlg_2020, 'train', 'original')\n",
        "dataset_20_dev_orig = load_webnlg(webnlg_2020, 'dev', 'original')\n",
        "dataset_20_test_orig = load_webnlg(webnlg_2020, 'test', 'original')\n",
        "\n",
        "# One data point is 3 lists: triples, english texts, irish texts\n",
        "# 1 triple example: [['Arianespace | country | France'], ['Arianespace is located in France.'], ['Tá Arianspace lonnaithe sa Fhrainc.']]\n",
        "dataset_23_train = []\n",
        "\n",
        "# Build WebNLG 2023 data structure like the one from HuggingFace for the other datasets\n",
        "# for entry in dataset_23_dict['benchmark']['entries']['entry']:\n",
        "#   data_point = []\n",
        "#   mtriples_list = []\n",
        "#   text_list_en = []\n",
        "#   text_list_ga = []\n",
        "#   # Get modified triples\n",
        "#   if isinstance(entry['modifiedtripleset']['mtriple'], list):\n",
        "#     for mtriple in entry['modifiedtripleset']['mtriple']:\n",
        "#       mtriples_list.append(mtriple)\n",
        "#   else:\n",
        "#     mtriples_list.append(entry['modifiedtripleset']['mtriple'])\n",
        "#   # Get reference texts\n",
        "#   for lex in entry['lex']:\n",
        "#     if lex['@lang'] == 'ga':\n",
        "#       text_list_ga.append(lex['#text'])\n",
        "#     else:\n",
        "#       text_list_en.append(lex['#text'])\n",
        "#   data_point.append(mtriples_list)\n",
        "#   data_point.append(text_list_en)\n",
        "#   data_point.append(text_list_ga)\n",
        "#   dataset_23_train.append(data_point)\n",
        "\n",
        "dataset_20_train_cat = load_webnlg_cat(webnlg_2020, 'train', 'original')\n",
        "dataset_20_dev_cat = load_webnlg_cat(webnlg_2020, 'dev', 'original')\n",
        "dataset_20_test_cat = load_webnlg_cat(webnlg_2020, 'test', 'original')\n",
        "\n",
        "# print(dataset_23_train\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KhLQBmCx8dFX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print examples\n",
        "print('WebNLG-2017')\n",
        "print('  train:', len(dataset_17_train))\n",
        "print(' ', dataset_17_train[2222])\n",
        "print('WebNLG-2018 (Enhanced)')\n",
        "print('  train:', len(dataset_18_train))\n",
        "print(' ', dataset_18_train[2222])\n",
        "print('WebNLG-2020')\n",
        "print('  train:', len(dataset_20_train))\n",
        "print('  test:', len(dataset_20_test))\n",
        "print('  dev:', len(dataset_20_dev))\n",
        "print(' ', dataset_20_train[2222])\n",
        "# print('WebNLG-2023')\n",
        "# print('  train:', len(dataset_23_train))\n",
        "# print(' ', dataset_23_train[2222])\n",
        "print(' ', dataset_20_train_cat[2222])\n",
        "\n",
        "print('WebNLG-2020_orig')\n",
        "print('  train:', len(dataset_20_train_orig))\n",
        "print('  test:', len(dataset_20_test_orig))\n",
        "print('  dev:', len(dataset_20_dev_orig))\n",
        "print(' ', dataset_20_train_orig[2222])"
      ],
      "metadata": {
        "id": "iyhBMHJ08mJZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get examples for properties\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "filepath = '/content/230528-WebNLG23_EN-GA_properties.txt'\n",
        "\n",
        "# a line contains semantically equivalent props separated by a vertical bar\n",
        "# 1stRunwayLengthFeet|1stRunwayLengthMetre|1st_runway_LengthFeet\n",
        "lines_props = codecs.open(filepath, 'r', 'utf-8').readlines()\n",
        "\n",
        "def getExamples(props_list, dataset):\n",
        "  # property_examples will contain 380 lists of 3 lists: 1- prop names, 2- verb en, 3 verb ga\n",
        "  property_examples = []\n",
        "  for props in props_list:\n",
        "    mini_list = []\n",
        "    equivalent_props = props.strip().split('|')\n",
        "    for equivalent_prop in equivalent_props:\n",
        "      mini_list.append(equivalent_prop)\n",
        "    property_examples.append([mini_list,[],[],[]])\n",
        "  # At this point, property_examples looks like this:\n",
        "  # [[['1stRunwayLengthFeet', '1stRunwayLengthMetre', '1st_runway_LengthFeet'], [], []], [['1stRunwayNumber', '1st_runway_Number'], [], []],...]\n",
        "\n",
        "  for entry in dataset:\n",
        "    # try to get verbalisations of one triple only\n",
        "    if len(entry[0]) == 1:\n",
        "      for input_triple in entry[0]:\n",
        "        prop_name = input_triple.split(' | ')[1]\n",
        "        for property_example in property_examples:\n",
        "          if prop_name in property_example[0]:\n",
        "            if len(property_example[1]) == 0:\n",
        "              property_example[1].append(input_triple)\n",
        "              property_example[2].append(entry[1][0])\n",
        "            if len(property_example[3]) == 0:\n",
        "              property_example[3].append(entry[2][0])\n",
        "\n",
        "  return(property_examples)\n",
        "\n",
        "my_examples = getExamples(lines_props, dataset_23_train)\n",
        "\n",
        "for example in my_examples:\n",
        "  print(example)"
      ],
      "metadata": {
        "id": "Vjgqb1PZ1cvt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get texts for translation\n",
        "# list_texts will look like this: [[[EN-sents0], [GA-sents0]], [[EN-sents1], [GA-sents1]], [[EN-sents2], [GA-sents2]], etc. ]\n",
        "# [[['The Aarhus is the airport of Aarhus, Denmark.', 'Aarhus Airport serves the city of Aarhus, Denmark.'], ['Is aerfoirt Aarhus, an Danmhairg.', 'Aerfort Aarhus seirbhísíonn an bhaile Aarhus, an Danmhairg.']],...]\n",
        "list_texts_ENGA = []\n",
        "for id, entry in enumerate(dataset_23_train):\n",
        "  # print(id)\n",
        "  list_texts_ENGA.append([])\n",
        "  list_texts_ENGA[id].append(entry[1])\n",
        "  list_texts_ENGA[id].append(entry[2])\n",
        "\n",
        "# to store one English text per input\n",
        "texts_en_single = []\n",
        "# to store all English texts for each input\n",
        "texts_en_all = []\n",
        "for texts_ENGA in list_texts_ENGA:\n",
        "  # texts_ENGA contains the EN and GA texts for one input; texts_ENGA[0] contains the English texts\n",
        "  texts_en_single.append(texts_ENGA[0][0])\n",
        "  for text_en in texts_ENGA[0]:\n",
        "    texts_en_all.append(text_en)\n",
        "\n",
        "# print(len(texts_en_single))\n",
        "# print(len(texts_en_all))\n",
        "\n",
        "def count_words(dataset):\n",
        "  count_words_all = 0\n",
        "  for text in dataset:\n",
        "    count_words_text = 0\n",
        "    text_split = text.split(' ')\n",
        "    for word in text_split:\n",
        "      count_words_text += 1\n",
        "    count_words_all += count_words_text\n",
        "  return(count_words_all)\n",
        "\n",
        "words_in_text_single = count_words(texts_en_single)\n",
        "words_in_text_all = count_words(texts_en_all)\n",
        "\n",
        "print(f'There are {words_in_text_single:,} words in the single-ref dataset ({words_in_text_single/len(texts_en_single)} words per text.)')\n",
        "print(f'There are {words_in_text_all:,} words in the multi-ref dataset ({words_in_text_all/len(texts_en_all)} words per text.)')\n",
        "\n",
        "# print(len(texts_en_all))"
      ],
      "metadata": {
        "id": "Jh56_H_1qfjq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get all property combinations for entities of each category (download manually when over)\n",
        "# For Sem accuracy experiments, I need a dico in which for each WebNLG category, I have all possible input configurations, so as to use these as templates to create new inputs that mirror the WebNLG configurations.\n",
        "import json\n",
        "\n",
        "# We want to group all categories that belong to a person\n",
        "# Note: artists can be bands or people. Included here because anyway some properties of e.g. Athletes don't apply to other people\n",
        "dico_map_categories = {'Artist':'Person', 'Astronaut':'Person', 'Athlete':'Person', 'Politician':'Person'}\n",
        "\n",
        "dico_category_tripleConfigs = {}\n",
        "for datapoint in dataset_20_train_cat:\n",
        "  # datapoint is a list with 3 elements: triples, texts, category: [['Arianespace | locationCountry | France'], ['Arianespace is located in France.'], 'MeanOfTransportation']\n",
        "  # Get dico key\n",
        "  category_triple = None\n",
        "  if datapoint[2] in dico_map_categories:\n",
        "    category_triple = dico_map_categories[datapoint[2]]\n",
        "  else:\n",
        "    category_triple = datapoint[2]\n",
        "  # Create key in output dico\n",
        "  if category_triple not in dico_category_tripleConfigs:\n",
        "    dico_category_tripleConfigs[category_triple] = {}\n",
        "  list_properties = []\n",
        "  for triple in datapoint[0]:\n",
        "    property_label = triple.split(' | ')[1]\n",
        "    list_properties.append(property_label)\n",
        "  str_list_props = '##'.join(sorted(list_properties))\n",
        "  if str_list_props not in dico_category_tripleConfigs[category_triple].keys():\n",
        "    dico_category_tripleConfigs[category_triple][str_list_props] = 1\n",
        "  else:\n",
        "    dico_category_tripleConfigs[category_triple][str_list_props] += 1\n",
        "\n",
        "# Sort combinations by frequency\n",
        "for key in dico_category_tripleConfigs:\n",
        "  dico_category_tripleConfigs[key] = dict(sorted(dico_category_tripleConfigs[key].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Save dico_category_tripleConfigs in a json\n",
        "with open(\"dico_category_tripleConfigs.json\", \"w\") as outfile:\n",
        "  json.dump(dico_category_tripleConfigs, outfile)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ESo1k819mcKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare properties of different datasets"
      ],
      "metadata": {
        "id": "0BvThbbOWguw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def comparePropertyLists(list1, list2):\n",
        "  property_not_found_1to2 = []\n",
        "  property_not_found_2to1 = []\n",
        "  for prop in list1:\n",
        "    if prop not in list2:\n",
        "      property_not_found_1to2.append(prop)\n",
        "  for prop in list2:\n",
        "    if prop not in list1:\n",
        "      property_not_found_2to1.append(prop)\n",
        "  return(property_not_found_1to2, property_not_found_2to1)\n",
        "\n",
        "properties_17_train = extractTripleElements(dataset_17_train, 'property')\n",
        "properties_18_train = extractTripleElements(dataset_18_train, 'property')\n",
        "properties_20_train = extractTripleElements(dataset_20_train, 'property')\n",
        "properties_23_train = extractTripleElements(dataset_23_train, 'property')\n",
        "\n",
        "_17notIn18, _18notIn17 = comparePropertyLists(properties_17_train, properties_18_train)\n",
        "# _17notIn20, _20notIn17 = comparePropertyLists(properties_17, properties_20)\n",
        "# _17notIn23, _23notIn17 = comparePropertyLists(properties_17, properties_23)\n",
        "_18notIn20, _20notIn18 = comparePropertyLists(properties_18_train, properties_20_train)\n",
        "_18notIn23, _23notIn18 = comparePropertyLists(properties_18_train, properties_23_train)\n",
        "_20notIn23, _23notIn20 = comparePropertyLists(properties_20_train, properties_23_train)"
      ],
      "metadata": {
        "id": "Hz7DXwqP9Bzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"WebnLG 17 VS Enhanced WebNLG\\n-----------------------------\")\n",
        "if len(_17notIn18) > 0:\n",
        "  print(\"Properties not found in 18:\")\n",
        "  print(sorted(_17notIn18))\n",
        "else:\n",
        "  print(\"All properties from WebNLG 17 are in Enhanced WebNLG\")\n",
        "\n",
        "if len(_18notIn17) > 0:\n",
        "  print(\"Properties not found in 17:\")\n",
        "  print(sorted(_18notIn17))\n",
        "else:\n",
        "  print(\"All properties from Enhanced WebNLG are in WebNLG 17\")\n",
        "\n",
        "# print()\n",
        "# print(\"WebnLG 17 VS WebNLG 20\\n-----------------------------\")\n",
        "# if len(_17notIn20) > 0:\n",
        "#   print(\"Properties not found in 20:\")\n",
        "#   print(sorted(_17notIn20))\n",
        "# else:\n",
        "#   print(\"All properties from WebNLG 17 are in WebNLG 20\")\n",
        "\n",
        "# if len(_20notIn17) > 0:\n",
        "#   print(\"Properties not found in 17:\")\n",
        "#   print(sorted(_20notIn17))\n",
        "# else:\n",
        "#   print(\"All properties from WebNLG 20 are in WebNLG 17\")\n",
        "\n",
        "print()\n",
        "print(\"Enhanced WebNLG VS WebnLG 20\\n-----------------------------\")\n",
        "if len(_18notIn20) > 0:\n",
        "  print(\"Properties not found in 20:\")\n",
        "  print(sorted(_18notIn20))\n",
        "else:\n",
        "  print(\"All properties from Enhanced WebNLG are in WebNLG 20\")\n",
        "\n",
        "if len(_20notIn18) > 0:\n",
        "  print(\"Properties not found in Enhanced:\")\n",
        "  print(sorted(_20notIn18))\n",
        "else:\n",
        "  print(\"All properties from WebNLG 20 are in Enhanced WebNLG\")\n",
        "\n",
        "print()\n",
        "print(\"Enhanced WebNLG VS WebnLG 23\\n-----------------------------\")\n",
        "if len(_18notIn23) > 0:\n",
        "  print(\"Properties not found in 23:\")\n",
        "  print(sorted(_18notIn23))\n",
        "else:\n",
        "  print(\"All properties from Enhanced WebNLG are in WebNLG 23\")\n",
        "\n",
        "if len(_23notIn18) > 0:\n",
        "  print(\"Properties not found in Enhanced:\")\n",
        "  print(sorted(_23notIn18))\n",
        "else:\n",
        "  print(\"All properties from WebNLG 23 are in Enhanced WebNLG\")\n",
        "\n",
        "print()\n",
        "print(\"WebnLG 20 VS WebnLG 23\\n-----------------------------\")\n",
        "if len(_20notIn23) > 0:\n",
        "  print(\"Properties not found in 23:\")\n",
        "  print(sorted(_20notIn23))\n",
        "else:\n",
        "  print(\"All properties from WebNLG 20 are in WebNLG 23\")\n",
        "\n",
        "if len(_23notIn20) > 0:\n",
        "  print(\"Properties not found in 20:\")\n",
        "  print(sorted(_23notIn20))\n",
        "else:\n",
        "  print(\"All properties from WebNLG 23 are in WebNLG 20\")"
      ],
      "metadata": {
        "id": "h-87dHk5DM1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get mappings between properties"
      ],
      "metadata": {
        "id": "DKu-LjO5-Doe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get mappings original - modified - Wikidata\n",
        "# Outcome of this cell: There is a major problem when looking for mappings between original and modified property labels, in that the triples are very often but not necessarily aligned in the \"original\" and \"modified\" fields of the WebNLG data.\n",
        "# Possible solution: filter out mappings that happen only once, since they are probably coming from data errors.\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Get mapping from a dbpedia property to a wikidata one; I already compiled a similar list in queryDBpediaProps.py\n",
        "# wkd_id_list = get_Wikidata_id_property('http://dbpedia.org/ontology/birthDate')\n",
        "# print(wkd_id_list)\n",
        "\n",
        "def compare_Orig_Modif_props(dataset_orig, dataset_modif, dico_mappings):\n",
        "  # x is a counter for triple sets\n",
        "  x = 0\n",
        "  while x < len(dataset_orig):\n",
        "    # print(f'#### Datapoint {x} ####')\n",
        "    triples_orig = dataset_orig[x][0]\n",
        "    triples_modif = dataset_modif[x][0]\n",
        "    # y is a counter for triples\n",
        "    y = 0\n",
        "    while y < len(triples_orig):\n",
        "      subj_orig_y = triples_orig[y].split(' | ')[0]\n",
        "      prop_orig_y = triples_orig[y].split(' | ')[1]\n",
        "      obj_orig_y = triples_orig[y].split(' | ')[2]\n",
        "      subj_modif_y = triples_modif[y].split(' | ')[0]\n",
        "      prop_modif_y = triples_modif[y].split(' | ')[1]\n",
        "      obj_modif_y = triples_modif[y].split(' | ')[2]\n",
        "\n",
        "      # In dico_mappings, let's have: {orig_prop1 {modif_prop1.1:count1.1, modif_prop1.2:count1.2, etc.}, orig_prop2 {modif_prop2.1:count2.1, modif_prop2.2:count2.2, etc.}}\n",
        "      if prop_orig_y not in dico_mappings:\n",
        "        dico_mappings[prop_orig_y] = {}\n",
        "        dico_mappings[prop_orig_y][prop_modif_y] = 1\n",
        "      else:\n",
        "        if prop_modif_y not in dico_mappings[prop_orig_y]:\n",
        "          dico_mappings[prop_orig_y][prop_modif_y] = 1\n",
        "        else:\n",
        "          dico_mappings[prop_orig_y][prop_modif_y] += 1\n",
        "\n",
        "      y += 1\n",
        "    x += 1\n",
        "\n",
        "  return dico_mappings\n",
        "\n",
        "def analyse_mappings(dico_mappings):\n",
        "  list_mappings_same = []\n",
        "  for prop_orig in dico_mappings:\n",
        "    # print(prop_orig, dico_mappings[prop_orig])\n",
        "    if len(dico_mappings[prop_orig]) == 1:\n",
        "      for prop_modif in dico_mappings[prop_orig]:\n",
        "        if prop_modif == prop_orig:\n",
        "          list_mappings_same.append(prop_orig)\n",
        "  list_mappings_same_sorted = sorted(list(set(list_mappings_same)))\n",
        "  return list_mappings_same_sorted\n",
        "\n",
        "\n",
        "dico_mappings_O2M = {}\n",
        "# Fill up dico_mappings\n",
        "dico_mappings_O2M = compare_Orig_Modif_props(dataset_20_dev_orig, dataset_20_dev, dico_mappings_O2M)\n",
        "dico_mappings_O2M = compare_Orig_Modif_props(dataset_20_train_orig, dataset_20_train, dico_mappings_O2M)\n",
        "dico_mappings_O2M = compare_Orig_Modif_props(dataset_20_test_orig, dataset_20_test, dico_mappings_O2M)\n",
        "with open(\"dico_map_Orig2Modif.json\", \"w\") as outfile:\n",
        "  json.dump(dico_mappings_O2M, outfile)\n",
        "\n",
        "dico_mappings_M2O = {}\n",
        "# Fill up dico_mappings\n",
        "dico_mappings_M2O = compare_Orig_Modif_props(dataset_20_dev, dataset_20_dev_orig, dico_mappings_M2O)\n",
        "dico_mappings_M2O = compare_Orig_Modif_props(dataset_20_train, dataset_20_train_orig, dico_mappings_M2O)\n",
        "dico_mappings_M2O = compare_Orig_Modif_props(dataset_20_test, dataset_20_test_orig, dico_mappings_M2O)\n",
        "with open(\"dico_map_Modif2Orig.json\", \"w\") as outfile:\n",
        "  json.dump(dico_mappings_M2O, outfile)\n",
        "\n",
        "# Check what the mappings look like\n",
        "list_mappings_straight = analyse_mappings(dico_mappings_O2M)\n",
        "print(f'\\n\\nThere are {len(list_mappings_straight)} properties that have always the same label in Orig and Modif vocabularies.\\n  {str(sorted(list_mappings_straight))}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r_74XkzW-Bze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check whether we have all WebNLG properties (original and Modified) in our predArg template file.\n",
        "# The idea is that we want to be able to use the generator on real DBpedia properties, without going through the modified propertiy labels. I'm pretty sure I did not check that all original property labels are mapped to something.\n",
        "# Actually checking the \"240202_WebNLG23_EN-GA_properties.txt\" file, which contains all properties according to the \"#@title Check predArg template files : Do NOT edit!\" cell of the Small_codes Notebook.\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "dico_map_Orig2Modif = json.load(open(\"dico_map_Orig2Modif.json\"))\n",
        "dico_map_Modif2Orig = json.load(open(\"dico_map_Modif2Orig.json\"))\n",
        "webnlg_properties_file = codecs.open('/content/240202_WebNLG23_EN-GA_properties.txt', 'r', 'utf-8').readlines()\n",
        "\n",
        "# webnlg_properties_templates_full will contain full names used in the template mappings (e.g. \"position[Subject_eq_Person]\"). It is used toi check for duplicates.\n",
        "webnlg_properties_templates_full = []\n",
        "# webnlg_properties_templates will contain the \"normal\" property names (e.g. \"position\")\n",
        "webnlg_properties_templates = []\n",
        "for line in webnlg_properties_file:\n",
        "  prop_names_list = line.strip().split('|')\n",
        "  for prop_name in prop_names_list:\n",
        "    clean_prop_name = prop_name.split('[')[0]\n",
        "    if prop_name not in webnlg_properties_templates_full:\n",
        "      webnlg_properties_templates_full.append(prop_name)\n",
        "    else:\n",
        "      print(f'Found duplicate property name: {clean_prop_name}')\n",
        "    if clean_prop_name not in webnlg_properties_templates:\n",
        "      webnlg_properties_templates.append(clean_prop_name)\n",
        "\n",
        "webnlg_properties_original = list(dico_map_Orig2Modif.keys())\n",
        "webnlg_properties_modified = list(dico_map_Modif2Orig.keys())\n",
        "\n",
        "print(f'Properties from templates: {len(webnlg_properties_templates)}')\n",
        "print(f'Properties from WebNLG-original: {len(webnlg_properties_original)}')\n",
        "print(f'Properties from WebNLG-modified: {len(webnlg_properties_modified)}')\n",
        "print('\\n')\n",
        "\n",
        "count_miss_modif = 0\n",
        "count_miss_orig = 0\n",
        "for wpm in webnlg_properties_modified:\n",
        "  if wpm not in webnlg_properties_templates:\n",
        "    count_miss_modif += 1\n",
        "    print(f'Property {wpm} from WebNLG-Modified is not in templates')\n",
        "print('\\n')\n",
        "for wpo in webnlg_properties_original:\n",
        "  if wpo not in webnlg_properties_templates:\n",
        "    count_miss_orig += 1\n",
        "    # Get most likely mapping according to dico_mapping\n",
        "    most_likely_mapping = max(dico_map_Orig2Modif[wpo], key=dico_map_Orig2Modif[wpo].get)\n",
        "    print(f'Property {wpo} from WebNLG-Original is not in templates.\\n  Modified: {most_likely_mapping} is the most likely mapping.')\n",
        "print('\\n')\n",
        "\n",
        "print(f'{count_miss_modif} modified properties are not covered.')\n",
        "print(f'{count_miss_orig} original properties are not covered.')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "R-YH8IVMgo4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check how many of the actual dbpedia properties we are currently covering, assuming all original properties are mapped to a predArg template (see previous cell).\n",
        "# For this let's use the list of original property labels extracted from the WebNLG dataset, and the list of all DBpedia properties obtained with a cell of the Small_codes notebook (see C:\\Users\\sfmil\\Desktop\\DCU-24\\2025-2026_ADAPT\\MyPapers\\2025-01_FORGe)\n",
        "\n",
        "dico_map_Orig2Modif = json.load(open(\"/content/dico_map_Orig2Modif.json\"))\n",
        "dico_map_Modif2Orig = json.load(open(\"/content/dico_map_Modif2Orig.json\"))\n",
        "dico_dbp_props = json.load(open(\"/content/dico_count_occurrences_dbp_props.json\"))\n",
        "\n",
        "webnlg_properties_original = list(dico_map_Orig2Modif.keys())\n",
        "webnlg_properties_modified = list(dico_map_Modif2Orig.keys())\n",
        "# dico_dbp_props is ordered by count, so the most frequent properties will appear first in the dbpedia_properties lists\n",
        "dbpedia_properties_all = [key.rsplit('/', 1)[1] for key in dico_dbp_props.keys()]\n",
        "dbpedia_properties_at_least_one_instance = [key.rsplit('/', 1)[1] for key in dico_dbp_props.keys() if dico_dbp_props[key] > 0]\n",
        "dbpedia_properties_zero_instance = [key.rsplit('/', 1)[1] for key in dico_dbp_props.keys() if dico_dbp_props[key] == 0]\n",
        "\n",
        "print(len(webnlg_properties_original), webnlg_properties_original)\n",
        "print(len(webnlg_properties_modified), webnlg_properties_modified)\n",
        "print(len(dbpedia_properties_at_least_one_instance), dbpedia_properties_at_least_one_instance)\n",
        "print(len(dbpedia_properties_zero_instance), dbpedia_properties_zero_instance)\n",
        "\n",
        "def compare_property_lists(candidate_list, dbpedia_properties_zero_instance, dbpedia_properties_at_least_one_instance):\n",
        "  props_in_dbpedia_zero_instance = []\n",
        "  props_in_dbpedia_at_least_one_instance = []\n",
        "  props_not_in_dbpedia = []\n",
        "\n",
        "  for webnlg_property in candidate_list:\n",
        "    if webnlg_property in dbpedia_properties_zero_instance:\n",
        "      props_in_dbpedia_zero_instance.append(webnlg_property)\n",
        "    elif webnlg_property in dbpedia_properties_at_least_one_instance:\n",
        "      props_in_dbpedia_at_least_one_instance.append(webnlg_property)\n",
        "    else:\n",
        "      props_not_in_dbpedia.append(webnlg_property)\n",
        "  return props_in_dbpedia_zero_instance, props_in_dbpedia_at_least_one_instance, props_not_in_dbpedia\n",
        "\n",
        "print('\\n')\n",
        "props_in_dbpedia_zero_instance_o, props_in_dbpedia_at_least_one_instance_o, props_not_in_dbpedia_o = compare_property_lists(webnlg_properties_original, dbpedia_properties_zero_instance, dbpedia_properties_at_least_one_instance)\n",
        "print(f'{len(props_in_dbpedia_zero_instance_o)} Original WebNLG properties have 0 instances in DBpedia.', sorted(props_in_dbpedia_zero_instance_o))\n",
        "print(f'{len(props_in_dbpedia_at_least_one_instance_o)} Original WebNLG properties have at least one instance in DBpedia.', sorted(props_in_dbpedia_at_least_one_instance_o))\n",
        "print(f'{len(props_not_in_dbpedia_o)} Original WebNLG properties are not in DBpedia.', sorted(props_not_in_dbpedia_o))\n",
        "print('\\n')\n",
        "\n",
        "props_in_dbpedia_zero_instance_m, props_in_dbpedia_at_least_one_instance_m, props_not_in_dbpedia_m = compare_property_lists(webnlg_properties_modified, dbpedia_properties_zero_instance, dbpedia_properties_at_least_one_instance)\n",
        "\n",
        "m_notIn_o_atLeastOneInstance = []\n",
        "m_notIn_o_zeroInstance = []\n",
        "for prop_in_dbpedia_at_least_one_instance_m in props_in_dbpedia_at_least_one_instance_m:\n",
        "  if prop_in_dbpedia_at_least_one_instance_m not in props_in_dbpedia_at_least_one_instance_o:\n",
        "    m_notIn_o_atLeastOneInstance.append(prop_in_dbpedia_at_least_one_instance_m)\n",
        "\n",
        "for prop_in_dbpedia_zero_instance_m in props_in_dbpedia_zero_instance_m:\n",
        "  if prop_in_dbpedia_zero_instance_m not in props_in_dbpedia_zero_instance_o:\n",
        "    m_notIn_o_zeroInstance.append(prop_in_dbpedia_zero_instance_m)\n",
        "\n",
        "print(f'{len(m_notIn_o_zeroInstance)} Modified WebNLG properties that are different from Original WebNLG properties have 0 instances in DBpedia.', sorted(m_notIn_o_zeroInstance))\n",
        "print(f'{len(m_notIn_o_atLeastOneInstance)} Modified WebNLG properties that are different from Original WebNLG properties have at least one instance in DBpedia.', sorted(m_notIn_o_atLeastOneInstance))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kGppI0vW3PB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read file with all properties covered by FORGe\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "props_list_path = os.path.join('/content', 'DCU_TCD_FORGe_WebNLG23', 'code', 'sorted_properties.txt')\n",
        "fd = codecs.open(props_list_path, 'r', 'utf-8')\n",
        "lines_properties = fd.readlines()\n",
        "list_properties = []\n",
        "for line_properties in lines_properties:\n",
        "  line_prop_list = line_properties.strip().split('-')\n",
        "  for prop in line_prop_list:\n",
        "    list_properties.append(prop)\n",
        "\n",
        "print(len(list_properties))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5P8yd0twBhkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get subject and object values and Wikidata Q-ID"
      ],
      "metadata": {
        "id": "0wlGgCYRVs04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create lists all entities\n",
        "import codecs\n",
        "\n",
        "subjects20_train = extractTripleElements(dataset_20_train, 'subject')\n",
        "subjects20_dev = extractTripleElements(dataset_20_dev, 'subject')\n",
        "subjects20_test = extractTripleElements(dataset_20_test, 'subject')\n",
        "objects20_train = extractTripleElements(dataset_20_train, 'object')\n",
        "objects20_dev = extractTripleElements(dataset_20_dev, 'object')\n",
        "objects20_test = extractTripleElements(dataset_20_test, 'object')\n",
        "properties20_train = extractTripleElements(dataset_20_train, 'property')\n",
        "properties20_dev = extractTripleElements(dataset_20_dev, 'property')\n",
        "properties20_test = extractTripleElements(dataset_20_test, 'property')\n",
        "\n",
        "def extend_elementList(list_all, new_list):\n",
        "  \"\"\"This function is supposed to be called the first time with an empty list as list_all\"\"\"\n",
        "  # print('Extension starts!')\n",
        "  for element in new_list:\n",
        "    if element not in list_all:\n",
        "      # print(element)\n",
        "      list_all.append(element)\n",
        "  return(list_all)\n",
        "\n",
        "all_properties = sorted(extend_elementList(extend_elementList(extend_elementList([], properties20_train), properties20_dev), properties20_test))\n",
        "all_subjects = sorted(extend_elementList(extend_elementList(extend_elementList([], subjects20_train), subjects20_dev), subjects20_test))\n",
        "all_objects = sorted(extend_elementList(extend_elementList(extend_elementList([], objects20_train), objects20_dev), objects20_test))\n",
        "all_entities = sorted(extend_elementList(extend_elementList([], all_subjects), all_objects))\n",
        "\n",
        "print(len(all_properties))\n",
        "print(len(all_subjects))\n",
        "print(len(all_objects))\n",
        "print(len(all_entities))"
      ],
      "metadata": {
        "id": "9AvEaY9TVxIg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save all_subjects and all_objects lists in text files, one word per line\n",
        "with open('all_subjects.txt', 'w', encoding='utf-8') as f:\n",
        "  for item in all_subjects:\n",
        "    f.write(f\"{item}\\n\")\n",
        "\n",
        "with open('all_objects.txt', 'w', encoding='utf-8') as f:\n",
        "  for item in all_objects:\n",
        "    f.write(f\"{item}\\n\")\n",
        "\n",
        "with open('all_properties.txt', 'w', encoding='utf-8') as f:\n",
        "  for item in all_properties:\n",
        "    f.write(f\"{item}\\n\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z1Zb4l10c_5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create lists filtered entities (UPLOAD FR_subj/obj_props_to_translate.txt)\n",
        "import codecs\n",
        "\n",
        "list_props_subj = [line.strip() for line in codecs.open('/content/FR_subj_props_to_translate.txt', 'r', 'utf-8').readlines()]\n",
        "list_props_obj = [line.strip() for line in codecs.open('/content/FR_obj_props_to_translate.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "# I compiled the next two lists manually to use them to find out which properties trigger their appearance in the lists of entities to be translated\n",
        "# subj_list_I_dont_want_them_translated_why_were_they = ['1089_Tama', '108_St_Georges_Terrace', 'AIDA_Cruises', 'Audi_A1', 'Rock_and_roll', 'The_Fellowship_of_the_Ring', 'The_Honeymoon_Killers_(American_band)', 'Train_(band)', 'A-Rosa Luna', 'AIDS (journal)', 'A Severed Wasp', 'Bananaman', 'Chinabank', 'English Without Tears', 'Let It Breed', 'Nord (Year of No Light album)', 'Soho Press', 'Sony Music Entertainment', 'Take It Off!']\n",
        "subj_list_I_dont_want_them_translated_why_were_they = ['Alcatraz_Versus_the_Evil_Librarians', 'A_Fortress_of_Grey_Ice', 'A_Long_Long_Way', 'A_Severed_Wasp', '1634:_The_Ram_Rebellion', '1634:_The_Bavarian_Crisis', '1634:_The_Baltic_War', 'Ring_of_Fire_II']\n",
        "# I was looking in the wrong file with all values instead of the file with filtered values only; the actual files look better, no big issues\n",
        "# obj_list_I_dont_want_them_translated_why_were_they = ['\"2\"', '\"03R/21L\"', '\"110 million (dollars)\"', '\"DL1, DL2, DL3\"', '0.0068 (kilometrePerSeconds)', '0.54 (square kilometres)', '1.1 (kilograms)', '1104.1 (inhabitants per square kilometre)', '1202.846 (days)', '125800.0 (millimetres)', '2.0 (gramPerCubicCentimetres)', 'Deșteaptă-te, române!', 'DeSoto Firedome', 'GMA New Media', \"Hook 'em (mascot)\", 'Kissing Spell Records', 'Lotus Eaters (band)', 'Mark Sixma', 'Marry Banilow', 'Max Benedict', 'Mike Akhigbe', 'Oberbürgermeister', 'Osmosys Records', 'Pools of Light (Brian Kelly album)', 'Riverside Art Museum', 'Roadside Attractions', 'Soho Press', 'South Capitol Street']\n",
        "\n",
        "def extractFilteredEntities(dataset, entity_type, list_properties):\n",
        "  \"\"\" Returns a list of subjects or objects, extracted from triple sets, filtered by property. e.g we want the subject values of the properties 'birthDate' and 'birthPlace' \"\"\"\n",
        "  n = ''\n",
        "  if entity_type == 'subject':\n",
        "    n = 0\n",
        "  elif entity_type == 'object':\n",
        "    n = 2\n",
        "  else:\n",
        "    print('Error, the second argument of extractTripleElements must be \"subject\", \"property\" or \"object\".')\n",
        "  element_list = []\n",
        "  for entry in dataset:\n",
        "    for input_triple in entry[0]:\n",
        "      property_name = input_triple.split(' | ')[1]\n",
        "      if property_name in list_properties:\n",
        "        element_name = input_triple.split(' | ')[n]\n",
        "        # For debugging\n",
        "        # Result: 55/66 errors come from the subject of \"location\", but in many cases it's better to translate the subj so don't act.\n",
        "        # if entity_type == 'subject' and element_name in subj_list_I_dont_want_them_translated_why_were_they:\n",
        "        #   print(f'  {element_name} is subject of {property_name}')\n",
        "        if element_name not in element_list:\n",
        "          element_list.append(element_name)\n",
        "  return(element_list)\n",
        "\n",
        "print('Extracting subjects from train')\n",
        "filt_subj20_train = extractFilteredEntities(dataset_20_train, 'subject', list_props_subj)\n",
        "print('Extracting subjects from dev')\n",
        "filt_subj20_dev = extractFilteredEntities(dataset_20_dev, 'subject', list_props_subj)\n",
        "print('Extracting subjects from test')\n",
        "filt_subj20_test = extractFilteredEntities(dataset_20_test, 'subject', list_props_subj)\n",
        "print('Extracting objects from train')\n",
        "filt_obj20_train = extractFilteredEntities(dataset_20_train, 'object', list_props_obj)\n",
        "print('Extracting objects from dev')\n",
        "fitl_obj20_dev = extractFilteredEntities(dataset_20_dev, 'object', list_props_obj)\n",
        "print('Extracting objects from test')\n",
        "filt_obj20_test = extractFilteredEntities(dataset_20_test, 'object', list_props_obj)\n",
        "# print(len(filtered_subjects_train), filtered_subjects_train)\n",
        "# print(len(filtered_objects_train), filtered_objects_train)\n",
        "filt_subjects = sorted(extend_elementList(extend_elementList(extend_elementList([], filt_subj20_train), filt_subj20_dev), filt_subj20_test))\n",
        "filt_objects = sorted(extend_elementList(extend_elementList(extend_elementList([], filt_obj20_train), fitl_obj20_dev), filt_obj20_test))\n",
        "filt_entities = sorted(extend_elementList(extend_elementList([], filt_subjects), filt_objects))\n",
        "\n",
        "print(len(filt_subjects))\n",
        "print(len(filt_objects))\n",
        "print(len(filt_entities))"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "ByWmpgXejCFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save filt_subjects and filt_objects lists in text files, one word per line\n",
        "with open('filtered_subjects.txt', 'w', encoding='utf-8') as f:\n",
        "  for item in filt_subjects:\n",
        "    f.write(f\"{item}\\n\")\n",
        "\n",
        "with open('filtered_objects.txt', 'w', encoding='utf-8') as f:\n",
        "  for item in filt_objects:\n",
        "    f.write(f\"{item}\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DXbWlrgoj2Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create CSV GEM QIDs\n",
        "rows = []\n",
        "header = ['Q-ID', 'Wikidata label', 'WebNLG label', 'Class_RegEx']\n",
        "get_wikidata_id_bulk(rows, all_entities, bar)\n",
        "\n",
        "with open('WebNLG_QIDs.csv', 'w', encoding='utf-8') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "  for row in rows:\n",
        "    writer.writerow(row)"
      ],
      "metadata": {
        "id": "Z9nfNz9QWFdS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEM shared task: Fix inputs and check uploaded files\n",
        "\n"
      ],
      "metadata": {
        "id": "eORSytr9VLej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install xmltodict"
      ],
      "metadata": {
        "id": "J2B0ubTwea2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fix inputs D2T\n",
        "import glob\n",
        "import os\n",
        "import codecs\n",
        "import re\n",
        "import random\n",
        "\n",
        "path_input_XML = '/content'\n",
        "list_XML_files = glob.glob(os.path.join(path_input_XML, '*[0-9].xml'))\n",
        "\n",
        "def getRandomListElementFromDict(dictListValue):\n",
        "  \"\"\" Returns a random element from a list when provided with a dico entry for which the value is this list\"\"\"\n",
        "  rand_num = random.randrange(len(dictListValue))\n",
        "  return(str(dictListValue[rand_num]))\n",
        "\n",
        "def write_line(out_file, new_line):\n",
        "  line_to_write = new_line\n",
        "  if re.search('^      \\r\\n', new_line) or re.search('^        \\r\\n', new_line):\n",
        "    pass\n",
        "  else:\n",
        "    if re.search('^      <entry ', new_line):\n",
        "      line_to_write = re.sub('^      <entry ', '    <entry ', new_line)\n",
        "    elif re.search('^          </entry>\\r\\n', new_line) or re.search('^      </entry>\\r\\n', new_line):\n",
        "      line_to_write = '    </entry>\\r\\n'\n",
        "    elif re.search('^            <modifiedtripleset>\\r\\n', new_line):\n",
        "      line_to_write = '      <modifiedtripleset>\\r\\n'\n",
        "    elif re.search('^            </modifiedtripleset>\\r\\n', new_line):\n",
        "      line_to_write = '      </modifiedtripleset>\\r\\n'\n",
        "    elif re.search('^              <mtriple>', new_line):\n",
        "      line_to_write = re.sub('^              <mtriple>', '        <mtriple>', new_line)\n",
        "    out_file.write(line_to_write)\n",
        "\n",
        "# For fictional wikidata dataset: DateOfBirth, DateOfDeath, EndOfWorkPeriod\n",
        "missing_values = { 'DateOfBirth' : ['1975-05-30', '2242-09-22', '1326-05-19', '119-02-24'], 'EndOfWorkPeriod' : ['1684', '3875', '1998', '318'], 'DateOfDeath' : ['1594-12-06', '2532-11-26', '1654-01-09', '221-09-09']}\n",
        "# For counterfactual webnlg dataset\n",
        "# For address, buildDate, gridReference, timeshiftChannel, training\n",
        "replace_obj_of = {'address' : ['103_Colmore_Row', '108_St_Georges_Terrace', '11_Diagonal_Street', '20_Fenchurch_Street', '200_Public_Square', '101 Ukrop Way'], 'builtDate' : ['1986-04-15', '2013-11-04', '1875-03-04', '1894-11-20', '1934-01-01', '2012-12-27'], 'gridReference' : ['NZ289147'], 'timeshiftChannel' : ['HBO_East,_HBO_West'], 'training' : ['School_of_Applied_Arts_in_Stuttgart'] }\n",
        "# For specific wrong object values\n",
        "# replace_value = {'\"In_Soldevanahalli_Acharya_' : {'creator' : ['Steve_Bright', 'Marie_Curie', 'Olga_Bondareva'], 'director' : ['Sarah_Teale', 'Virginia_DeMarce', 'Stacy_Katzman'], 'distributor' : ['Lionsgate', 'Alliance_Films_Corporation', 'Roadside_Attractions'], 'editing' : ['Stacy_Katzman', 'Max_Benedict'], 'gridReference' : ['NZ289147'], 'precededBy' : ['The_Hobbit', 'Let_It_Breed', 'This''ll_Be_My_Year'], 'president' : ['Stacy_Katzman', 'John_F._Kennedy', 'Virginia_DeMarce'], 'producer' : ['The_Velvet_Underground', 'Year_of_No_Light', 'Anatole_de_Grunwald'], 'recordLabel' : ['Polydor_Records', 'Columbia_Records', 'Sony_Music_Entertainment', 'Universal_Music_Group'], 'spouse' : ['Casey_Ribicoff', 'Steve_Bright', 'Marie_Curie'], 'timeshiftChannel' : ['HBO' 'East', 'HBO' 'West'], 'training' : ['School_of_Applied_Arts_in_Stuttgart'], 'type' : ['City', 'Compilation_Album', 'Public_company']}, '\"May_1950_-_August_1956' : {'campus' : ['Dijon', '\"In' 'Soldevanahalli', 'Acharya' 'Dr.' 'Sarvapalli' 'Radhakrishnan' 'Road', 'Hessarghatta' 'Main' 'Road', 'Bangalore' '–' '560090.\"'], 'creator' : ['Steve_Bright', 'Marie_Curie', 'Amund_Bjørklund'], 'designer' : ['Sarah_Teale', 'Virginia_DeMarce', 'Olga_Bondareva'], 'director' : ['Sarah_Teale', 'Marie_Curie', 'Stacy_Katzman'], 'editing' : ['Casey_Ribicoff', 'Stacy_Katzman', 'Max_Benedict'], 'musicSubgenre' : ['Southern_sludge', 'Proto-punk', 'Indie_pop'], 'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva'], 'training' : ['School_of_Applied_Arts_in_Stuttgart']}, 'andminus;7' :{'academicStaffSize' : ['500', '986', '4', '42'], 'editing' : ['Casey_Ribicoff', 'Stacy_Katzman', 'Max_Benedict'], 'meaning' : ['Opening_of_hope']}, 'School_of_Applied_Arts_in_Stuttgart' : {'background' : ['non_performing_personnel'], 'buildDate' : ['1986-04-15', '2013-11-04', '1875-03-04', '1894-11-20', '1934-01-01', '2012-12-27'], 'editing' : ['Robert_A._M._Stern', 'Amund_Bjørklund'], 'precededBy' : ['The_Hobbit', 'Let_It_Breed', \"This'll_Be_My_Year\"]}, 'Alliance_Films_Corporation' : {'award' : ['State_Award_for_Superior_Achievement', 'Distinguished_Service_Medal_(United_States_Navy)'], 'buildDate' : ['1986-04-15', '2013-11-04', '1875-03-04', '1894-11-20', '1934-01-01', '2012-12-27'], 'campus' : ['Dijon', '\"In_Soldevanahalli,_Acharya_Dr._Sarvapalli_Radhakrishnan_Road,_Hessarghatta_Main_Road,_Bangalore_–_560090.\"'], 'director' : ['Sarah_Teale', 'Virginia_DeMarce', 'Stacy_Katzman'], 'nickname' : ['Asa_Gigante', 'Alvinegro'], 'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva']}, '\"DeMarce_short_stories_in_the_The_Grantville_Gazettes' : {'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva']}, \"This'll_Be_My_Year\" : {'buildDate' : ['1986-04-15', '2013-11-04', '1875-03-04', '1894-11-20', '1934-01-01', '2012-12-27'], 'campus' : ['Dijon', '\"In_Soldevanahalli,_Acharya_Dr._Sarvapalli_Radhakrishnan_Road,_Hessarghatta_Main_Road,_Bangalore_–_560090.\"'], 'director' : ['Sarah_Teale', 'Virginia_DeMarce', 'Stacy_Katzman'], 'nickname' : ['Asa_Gigante', 'Alvinegro'], 'producer' : ['The_Velvet_Underground', 'Year_of_No_Light', 'Anatole_de_Grunwald'], 'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva'], 'training' : ['School_of_Applied_Arts_in_Stuttgart']}  }\n",
        "# Same as list above but removing the properties covered in replace_obj_of list and editing problematic values\n",
        "replace_value = {'\\\"In_Soldevanahalli_Acharya_' : {'creator' : ['Steve_Bright', 'Marie_Curie', 'Olga_Bondareva'], 'director' : ['Sarah_Teale', 'Virginia_DeMarce', 'Stacy_Katzman'], 'distributor' : ['Lionsgate', 'Alliance_Films_Corporation', 'Roadside_Attractions'], 'editing' : ['Stacy_Katzman', 'Max_Benedict'], 'precededBy' : ['The_Hobbit', 'Let_It_Breed'], 'president' : ['Stacy_Katzman', 'John_F._Kennedy', 'Virginia_DeMarce'], 'producer' : ['The_Velvet_Underground', 'Year_of_No_Light', 'Anatole_de_Grunwald'], 'recordLabel' : ['Polydor_Records', 'Columbia_Records', 'Sony_Music_Entertainment', 'Universal_Music_Group'], 'spouse' : ['Casey_Ribicoff', 'Steve_Bright', 'Marie_Curie'], 'type' : ['City', 'Compilation_Album', 'Public_company']}, '\"May_1950_-_August_1956' : {'campus' : ['Dijon', 'Bangalore_–_560090'], 'creator' : ['Steve_Bright', 'Marie_Curie', 'Amund_Bjørklund'], 'designer' : ['Sarah_Teale', 'Virginia_DeMarce', 'Olga_Bondareva'], 'director' : ['Sarah_Teale', 'Marie_Curie', 'Stacy_Katzman'], 'editing' : ['Casey_Ribicoff', 'Stacy_Katzman', 'Max_Benedict'], 'musicSubgenre' : ['Southern_sludge', 'Proto-punk', 'Indie_pop'], 'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva']}, 'andminus;7' :{'academicStaffSize' : ['500', '986', '4', '42'], 'editing' : ['Casey_Ribicoff', 'Stacy_Katzman', 'Max_Benedict'], 'meaning' : ['Opening_of_hope']}, 'School_of_Applied_Arts_in_Stuttgart' : {'background' : ['non_performing_personnel'], 'editing' : ['Robert_A._M._Stern', 'Amund_Bjørklund'], 'precededBy' : ['The_Hobbit', 'Let_It_Breed']}, 'Alliance_Films_Corporation' : {'award' : ['State_Award_for_Superior_Achievement', 'Distinguished_Service_Medal_(United_States_Navy)'], 'campus' : ['Dijon', 'Bangalore_–_560090'], 'director' : ['Sarah_Teale', 'Virginia_DeMarce', 'Stacy_Katzman'], 'nickname' : ['Asa_Gigante', 'Alvinegro'], 'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva']}, '\"DeMarce_short_stories_in_the_The_Grantville_Gazettes' : {'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva']}, 'This\\'ll_Be_My_Year' : {'campus' : ['Dijon', 'Bangalore_–_560090'], 'director' : ['Sarah_Teale', 'Virginia_DeMarce', 'Stacy_Katzman'], 'nickname' : ['Asa_Gigante', 'Alvinegro'], 'producer' : ['The_Velvet_Underground', 'Year_of_No_Light', 'Anatole_de_Grunwald'], 'spouse' : ['Steve_Bright', 'Amund_Bjørklund', 'Olga_Bondareva']} }\n",
        "\n",
        "# Lists to store how many fixes are made\n",
        "fixed_missing_obj = []\n",
        "fixed_missing_obj_file = []\n",
        "fixed_obj_of = []\n",
        "fixed_obj_of_file = []\n",
        "fixed_obj = []\n",
        "fixed_obj_file = []\n",
        "for XML_file_path in list_XML_files:\n",
        "  print(XML_file_path)\n",
        "  xml_file = codecs.open(XML_file_path, 'r', 'utf-8').readlines()\n",
        "  new_file_path = str(XML_file_path.rsplit('.', 1)[0])+'-EDIT.xml'\n",
        "  count_matches = 0\n",
        "  with codecs.open(new_file_path, 'w', 'utf-8') as fo:\n",
        "    # Fix missing objects, apply to all files\n",
        "    # if XML_file_path == '/content/fictional_wikidata_1900-2023-10-14.xml':\n",
        "    # This variable stores whether we are in a new input, so we generate a new value, or in an input for which a value has already been generated (we have otriples and mtriples that need to have the same values)\n",
        "    new_input_obj_of = 'yes'\n",
        "    new_input_obj = 'yes'\n",
        "    # I organise these as dicos because there can be several substitutions in the same input (right now I don't cover multiple substitutions of the same property)\n",
        "    stored_value_obj_of = {}\n",
        "    stored_value_obj = {}\n",
        "    for count_fix, line in enumerate(xml_file):\n",
        "      new_line = line\n",
        "      # print('LINE : \"'+line+'\"')\n",
        "      # Add missing properties\n",
        "      if re.search(' \\| <', line):\n",
        "        match_property = 'no'\n",
        "        for property_empty_obj in list(missing_values.keys()):\n",
        "          if re.search(property_empty_obj+' \\| <', line):\n",
        "            match_property = 'yes'\n",
        "            # Get a random value from the list\n",
        "            random_value = getRandomListElementFromDict(missing_values[property_empty_obj])\n",
        "            new_line = re.sub(' \\| <', ' | '+str(random_value)+'<', line)\n",
        "            fixed_missing_obj.append(count_fix)\n",
        "            if XML_file_path not in fixed_missing_obj_file:\n",
        "              fixed_missing_obj_file.append(XML_file_path)\n",
        "        # If there is no proposed value for the property, just write unknown\n",
        "        if match_property == 'no':\n",
        "          new_line = re.sub(' \\| <', ' | Unknown <', line)\n",
        "        print(new_line)\n",
        "      # write_line(fo, new_line)\n",
        "      # new_line = line\n",
        "      # Change the object values of all instances of a specific list of properties; only apply to webnlg counterfactual\n",
        "      else:\n",
        "        if XML_file_path == '/content/rdf-to-text-generation-test-data-with-refs-en-counterfactual-v0.7.xml':\n",
        "          for prop_replace_obj_of in list(replace_obj_of.keys()):\n",
        "            if re.search(' \\| '+str(prop_replace_obj_of)+' \\| ', line):\n",
        "              # If new input, we randomly select a value\n",
        "              if new_input_obj_of == 'yes':\n",
        "                random_value_obj_of = getRandomListElementFromDict(replace_obj_of[prop_replace_obj_of])\n",
        "                new_line = re.sub(' \\| [^<\\|]+<', ' | '+str(random_value_obj_of)+'<', line)\n",
        "                stored_value_obj_of[prop_replace_obj_of] = random_value_obj_of\n",
        "                new_input_obj_of = 'no'\n",
        "              # If seen input, reuse the value selected before\n",
        "              elif new_input_obj_of == 'no':\n",
        "                new_line = re.sub(' \\| [^<\\|]+<', ' | '+str(stored_value_obj_of[prop_replace_obj_of])+'<', line)\n",
        "              fixed_obj_of.append(count_fix)\n",
        "              if XML_file_path not in fixed_obj_of_file:\n",
        "                fixed_obj_of_file.append(XML_file_path)\n",
        "              print(new_line)\n",
        "        # Change specific object values if found with specific properties; apply to all files (excludes properties handled in the if above so there is no overlap between the 2)\n",
        "        for obj_value in list(replace_value.keys()):\n",
        "          if re.search(' \\| '+str(obj_value)+'[^<\\|]+<', line):\n",
        "            for target_property in list(replace_value[obj_value].keys()):\n",
        "              if re.search(' \\| '+str(target_property)+' \\| ', line):\n",
        "                # If new input, we randomly select a value\n",
        "                if new_input_obj == 'yes':\n",
        "                  random_value_obj = getRandomListElementFromDict(replace_value[obj_value][target_property])\n",
        "                  new_line = re.sub(' \\| [^<\\|]+<', ' | '+str(random_value_obj)+'<', line)\n",
        "                  stored_value_obj[target_property] = random_value_obj\n",
        "                  new_input_obj = 'no'\n",
        "                # If seen input, reuse the value selected before\n",
        "                elif new_input_obj == 'no':\n",
        "                  new_line = re.sub(' \\| [^<\\|]+<', ' | '+str(stored_value_obj[target_property])+'<', line)\n",
        "                fixed_obj.append(count_fix)\n",
        "                if XML_file_path not in fixed_obj_file:\n",
        "                  fixed_obj_file.append(XML_file_path)\n",
        "                print(new_line)\n",
        "      write_line(fo, new_line)\n",
        "      # When we see a new input, reset the variables\n",
        "      if re.search('    <entry category=', line):\n",
        "        new_input_obj_of = 'yes'\n",
        "        new_input_obj = 'yes'\n",
        "\n",
        "print('Fixed '+str(len(fixed_missing_obj))+' missing objects in '+str(fixed_missing_obj_file))\n",
        "print('Fixed '+str(len(fixed_obj_of))+' objects of a targeted prop in '+str(fixed_obj_of_file))\n",
        "print('Fixed '+str(len(fixed_obj))+' targeted objects in '+str(fixed_obj_file))"
      ],
      "metadata": {
        "id": "UZaAvYy7VPyl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check uploaded files\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "out_folder = 'GEM_test'\n",
        "\n",
        "paths_submissions = glob.glob(os.path.join(out_folder, '*'))\n",
        "\n",
        "def check_GEM_submissions(path_submission, sys_output):\n",
        "  # Check name\n",
        "  task_suffixes = ('_D2T-1-FA', '_D2T-1-FI', '_D2T-1-CFA', '_D2T-2-FA', '_D2T-2-FI', '_D2T-2-CFA', '_Summ-1', '_Summ-2', '_Summ-3')\n",
        "  d2t1_IDs = ['D2T-1-FA', 'D2T-1-FI', 'D2T-1-CFA']\n",
        "  d2t2_IDs = ['D2T-2-FA', 'D2T-2-FI', 'D2T-2-CFA']\n",
        "  summ_IDs = ['Summ-1', 'Summ-2', 'Summ-3']\n",
        "  languages = ('_en', '_zh', '_de', '_ru', '_es', '_ko', '_hi', '_sw', '_ar')\n",
        "  extensions = ('.txt', '.jsonl')\n",
        "\n",
        "  filename = os.path.basename(path_submission)\n",
        "  print(filename)\n",
        "\n",
        "  # Check extension\n",
        "  if filename.endswith(extensions):\n",
        "    filename_noExt = filename.rsplit('.', 1)[0]\n",
        "    extension = filename.rsplit('.', 1)[1]\n",
        "    # Check language ID\n",
        "    if filename_noExt.endswith(languages):\n",
        "      filename_noExt_noLang = filename_noExt.rsplit('_', 1)[0]\n",
        "      # Check task identifier\n",
        "      if filename_noExt_noLang.endswith(task_suffixes):\n",
        "        filename_noExt_noLang_noTask = filename_noExt_noLang.rsplit('_', 1)[0]\n",
        "        task_ID = filename_noExt_noLang.rsplit('_', 1)[1]\n",
        "        # If there is a system name, open the files and check inside\n",
        "        if len(filename_noExt_noLang_noTask) > 0:\n",
        "          # txt files are for the D2T task; D2T-1 should have 1,779 lines, D2T-2 should have 1,800 lines.\n",
        "          if extension == 'txt':\n",
        "            file_lines = sys_output.readlines()\n",
        "            # Check line numbers in D2T-1 data\n",
        "            if task_ID in d2t1_IDs and not len(file_lines) == 1779:\n",
        "              print(f'  Error line numbers!\\n\\t{filename} should have 1,779 lines (found {len(file_lines)}).')\n",
        "            # Check line numbers in D2T-2 data\n",
        "            elif task_ID in d2t2_IDs and not len(file_lines) == 1800:\n",
        "              print(f'  Error line numbers!\\n\\t{filename} should have 1,800 lines (found {len(file_lines)}).')\n",
        "            else:\n",
        "              print('  OK!')\n",
        "          # json files are for the summ task; check well-formedness\n",
        "          elif extension == 'json':\n",
        "            try:\n",
        "              json.load(sys_output)\n",
        "            except:\n",
        "              print(f'  Error json formatting! Check {filename_noExt}.')\n",
        "            # There should additional be code to check the number of outputs in the submitted files\n",
        "        else:\n",
        "          print(f'  Error filename system name!\\n\\t{filename_noExt} should have a name before the task suffix.')\n",
        "      else:\n",
        "          print(f'  Error filename task suffix!\\n\\t{filename_noExt} should contain one of these task suffixes: {task_suffixes}.')\n",
        "    else:\n",
        "      print(f'  Error filename language suffix!\\n\\t{filename_noExt} should end with one of these language suffixes: {languages}.')\n",
        "  else:\n",
        "    print(f'  Error filename extension!\\n\\t{filename} should have one of these extensions (according to task): {extensions}.')\n",
        "\n",
        "for path_submission in paths_submissions:\n",
        "  # We should receive \"path_submission\" an \"sys_output\"\n",
        "  sys_output = codecs.open(path_submission, 'r', 'utf-8')\n",
        "\n",
        "  check_GEM_submissions(path_submission, sys_output)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JGtEP8h5Thwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract one reference text per input in the test data\n",
        "import random\n",
        "import codecs\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "with codecs.open('0-references_D2T-1-FA_en.txt', 'w', 'utf-8') as fo:\n",
        "  # The test data on HuggingFace contains first the D2T data and then the Text-to-Triple data\n",
        "  for dtp_test in dataset_20_test[:1779]:\n",
        "    # Get the number of reference texts for each data point\n",
        "    num_refs = len(dtp_test[1])\n",
        "    # Get a random number within the range of the number of texts\n",
        "    id_select = random.randint(0, num_refs-1)\n",
        "    # print(num_refs, id_select)\n",
        "    fo.write(dtp_test[1][id_select])\n",
        "    fo.write('\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rj1TJ35aJyrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EqRMqO2Km7bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get French labels for WebNLG entities"
      ],
      "metadata": {
        "id": "vEQ6kkJWnTgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Examine existing files\n",
        "# We don't really remember what was done for Irish, so I want to understand what the different files used in FromTriples2PredArg contains and are for\n",
        "# Conclusions: all_obj and all_subj contain all other train/dev/test files.\n",
        "# Conclusions: objValuesTest_dbpediaTranslations is entirely contained in objValues_dbpediaTranslations except for 4 values that I think were cleaned manually.\n",
        "import codecs\n",
        "\n",
        "# train_obj = [line.strip() for line in codecs.open('/content/train_objValues.txt', 'r', 'utf-8').readlines()]\n",
        "# train_subj = [line.strip() for line in codecs.open('/content/train_subValues.txt', 'r', 'utf-8').readlines()]\n",
        "# test_obj = [line.strip() for line in codecs.open('/content/test_objValues.txt', 'r', 'utf-8').readlines()]\n",
        "# test_subj = [line.strip() for line in codecs.open('/content/test_subValues.txt', 'r', 'utf-8').readlines()]\n",
        "# dev_obj = [line.strip() for line in codecs.open('/content/dev_objValues.txt', 'r', 'utf-8').readlines()]\n",
        "# dev_subj = [line.strip() for line in codecs.open('/content/dev_subValues.txt', 'r', 'utf-8').readlines()]\n",
        "# all_obj = [line.strip() for line in codecs.open('/content/all_objValues.txt', 'r', 'utf-8').readlines()]\n",
        "# all_subj = [line.strip() for line in codecs.open('/content/all_subValues.txt', 'r', 'utf-8').readlines()]\n",
        "# dbpedia_obj = [line.strip() for line in codecs.open('/content/objValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "# dbpedia_objTest = [line.strip() for line in codecs.open('/content/objValuesTest_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "# dbpedia_obj = [line.strip() for line in codecs.open('/content/objValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "# translate_obj = [line.strip() for line in codecs.open('/content/objValues_googleTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "# dbpedia_subj = [line.strip() for line in codecs.open('/content/subValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "# translate_subj = [line.strip() for line in codecs.open('/content/subValues_googleTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "def check_overlap_lists(list1, list2):\n",
        "  \"\"\"Checks if all elements of list1 are found in list2\"\"\"\n",
        "  not_found = []\n",
        "  for element in list1:\n",
        "    if element not in list2:\n",
        "      not_found.append(element)\n",
        "  if len(not_found) == 0:\n",
        "    print('All elements of the first list are in the second list!')\n",
        "  elif len(not_found) == len(list1):\n",
        "    print('No overlap found between the two lists!')\n",
        "  else:\n",
        "    print(f'{len(not_found)} elements of the first list are not in the second list: {not_found}')\n",
        "\n",
        "# check_overlap_lists(dbpedia_subj, translate_subj)"
      ],
      "metadata": {
        "id": "Do3xWIlOnTgX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions to get DBpedia and Google translations\n",
        "# Adapted from ChatGPT: Please write some Python code which, given an English entity label (with underscores instead of spaces), retrieves the corresponding French label according to DBpedia.\n",
        "from googletrans import Translator\n",
        "import requests\n",
        "\n",
        "def get_translated_label_dbpedia(language, entity_label):\n",
        "  # Format entity to escape/remove all reserved charachers\n",
        "  formatted_entity_label = format_entity_dbp(entity_label.replace(' ', '_'))\n",
        "  # DBpedia SPARQL endpoint\n",
        "  sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
        "  # Construct the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT ?label WHERE {{\n",
        "    dbr:{formatted_entity_label} rdfs:label ?label .\n",
        "    FILTER (lang(?label) = '{language}')\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Send the request to the SPARQL endpoint\n",
        "  response = requests.get(sparql_endpoint, params={'query': query, 'format': 'json'})\n",
        "  data = response.json()\n",
        "\n",
        "  # Extract the French label from the results\n",
        "  try:\n",
        "    french_label = data['results']['bindings'][0]['label']['value']\n",
        "    return french_label\n",
        "  except (IndexError, KeyError):\n",
        "    return None  # Return None if no French label is found\n",
        "\n",
        "# Example usage\n",
        "# entity = \"People\\\\'s_Party_(Spain)\"  # Example entity\n",
        "# entity = \"Fried_chicken\"  # Example entity\n",
        "# french_label = get_translated_label_dbpedia('fr', entity)\n",
        "# print(f\"French label for {entity}: {french_label.replace(' ', '_')}\")\n",
        "\n",
        "translator = Translator()\n",
        "def get_translated_labels_google(dest_language, entity_list):\n",
        "  entity_list_with_spaces = [entity_label.replace('_', ' ') for entity_label in entity_list]\n",
        "  translations = translator.translate(entity_list_with_spaces, dest=dest_language)\n",
        "  list_transl = []\n",
        "  # We're not reintroducing underscores since that's what I initially sent to the triple2predarg module and I don't want to break anything\n",
        "  for translation in translations:\n",
        "    list_transl.append(translation.text.strip())\n",
        "  return list_transl"
      ],
      "metadata": {
        "id": "tTHzkJuonTgY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Translate entities from GA files using DBpedia or Google Translate if not found on DBpedia\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "dest_language = 'fr'#@param['fr', 'ga']\n",
        "\n",
        "# The files' lines are an english word and an irish word separated by an asterisk (asterisk possibly surrounded by tabs)\n",
        "# For Google translate, we need spaces between the words, not for DBpedia (the DBpedia function takes care of formatting)\n",
        "# Update: if we remove underscores here we're losing the alignment with the entities in the WebNLG inputs; now done within the Google function\n",
        "ga_dbpedia_obj = []\n",
        "ga_translate_obj = []\n",
        "ga_dbpedia_subj = []\n",
        "ga_translate_subj = []\n",
        "# If we use uploaded GA files as input\n",
        "if os.path.exists('/content/objValues_dbpediaTranslations.txt'):\n",
        "  print('Starting to process files coming from GA experiments...')\n",
        "  # WAS: ga_dbpedia_obj = [line.split('*')[0].replace('_', ' ').strip() for line in codecs.open('/content/objValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "  ga_dbpedia_obj = [line.split('*')[0].strip() for line in codecs.open('/content/objValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "  ga_translate_obj = [line.split('*')[0].strip() for line in codecs.open('/content/objValues_googleTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "  ga_dbpedia_subj = [line.split('*')[0].strip() for line in codecs.open('/content/subValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "  ga_translate_subj = [line.split('*')[0].strip() for line in codecs.open('/content/subValues_googleTranslations.txt', 'r', 'utf-8').readlines()]\n",
        "# if we use the subjects and objects collected from the WebNLG data using the cells above\n",
        "elif os.path.exists('/content/filtered_objects.txt'):\n",
        "  print('Starting to process files with only filtered WebNLG entities...')\n",
        "  ga_dbpedia_obj = [line.strip() for line in codecs.open('/content/filtered_objects.txt', 'r', 'utf-8').readlines()]\n",
        "  ga_dbpedia_subj = [line.strip() for line in codecs.open('/content/filtered_subjects.txt', 'r', 'utf-8').readlines()]\n",
        "elif os.path.exists('/content/all_objects.txt'):\n",
        "  print('Starting to process files with all WebNLG entities...')\n",
        "  ga_dbpedia_obj = [line.strip() for line in codecs.open('/content/all_objects.txt', 'r', 'utf-8').readlines()]\n",
        "  ga_dbpedia_subj = [line.strip() for line in codecs.open('/content/all_subjects.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "# These are the lists that will contain english and translated entitied aligned; we split files by subject/object and dbpedia/google because that's what the triple2predarg code expects\n",
        "list_gtrans_en_subj = []\n",
        "list_gtrans_en_obj = []\n",
        "list_gtrans_dest_subj = []\n",
        "list_gtrans_dest_obj = []\n",
        "list_dbp_en_subj = []\n",
        "list_dbp_en_obj = []\n",
        "list_dbp_dest_subj = []\n",
        "list_dbp_dest_obj = []\n",
        "\n",
        "def get_DBpedia_labels_and_list_for_gtrans(dest_language, list_en, list_dbp_en, list_dbp_dest, list_for_gtrans):\n",
        "  \"\"\"\n",
        "  This function retrieves  DBpedia labels in the target language, and creates a list with the remaining entities to be translated using an MT system.\n",
        "  Expects a list of english entities with no underscores in.\n",
        "  Returns three lists: two with the DBpedia labels (english and target language), and one and one with the entities not found on DBpedia\n",
        "  \"\"\"\n",
        "  for i, entity in enumerate(list_en):\n",
        "    print(f'  Processing entity {i+1}/{len(list_en)} - {entity}...')\n",
        "    result_query = get_translated_label_dbpedia(dest_language, entity)\n",
        "    if result_query == None:\n",
        "      list_for_gtrans.append(entity)\n",
        "    else:\n",
        "      # We need to reintroduce underscores for the entities to match a in the WebNLG inputs\n",
        "      list_dbp_en.append(entity.replace(' ', '_'))\n",
        "      list_dbp_dest.append(result_query.replace(' ', '_'))\n",
        "  #return list_dbp_en, list_dbp_dest, list_for_gtrans\n",
        "\n",
        "# Get lists for subject entities\n",
        "print('Getting lists for subject entities...')\n",
        "get_DBpedia_labels_and_list_for_gtrans(dest_language, ga_dbpedia_subj, list_dbp_en_subj, list_dbp_dest_subj, list_gtrans_en_subj)\n",
        "if os.path.exists('/content/objValues_dbpediaTranslations.txt'):\n",
        "  get_DBpedia_labels_and_list_for_gtrans(dest_language, ga_translate_subj, list_dbp_en_subj, list_dbp_dest_subj, list_gtrans_en_subj)\n",
        "# Translate subject entities not found on DBpedia\n",
        "if len(list_gtrans_en_subj) > 0:\n",
        "  list_gtrans_dest_subj = get_translated_labels_google(dest_language, list_gtrans_en_subj)\n",
        "print(f'Subjects: expected {len(ga_dbpedia_subj)+len(ga_translate_subj)} items (DBp: {len(ga_dbpedia_subj)}; GTr: {len(ga_translate_subj)}).')\n",
        "print(f'DBp en:\\t{len(list_dbp_en_subj)} {list_dbp_en_subj}')\n",
        "print(f'DBp {dest_language}:\\t{len(list_dbp_dest_subj)} {list_dbp_dest_subj}')\n",
        "print(f'GTr en:\\t{len(list_gtrans_en_subj)} {list_gtrans_en_subj}')\n",
        "print(f'GTr {dest_language}:\\t{len(list_gtrans_dest_subj)} {list_gtrans_dest_subj}')\n",
        "print('\\n')\n",
        "\n",
        "# Get lists for object entities\n",
        "print('Getting lists for object entities...')\n",
        "get_DBpedia_labels_and_list_for_gtrans(dest_language, ga_dbpedia_obj, list_dbp_en_obj, list_dbp_dest_obj, list_gtrans_en_obj)\n",
        "if os.path.exists('/content/objValues_dbpediaTranslations.txt'):\n",
        "  get_DBpedia_labels_and_list_for_gtrans(dest_language, ga_translate_obj, list_dbp_en_obj, list_dbp_dest_obj, list_gtrans_en_obj)\n",
        "# Translate object entities not found on DBpedia\n",
        "if len(list_gtrans_en_obj) > 0:\n",
        "  list_gtrans_dest_obj = get_translated_labels_google(dest_language, list_gtrans_en_obj)\n",
        "print(f'Objects: expected {len(ga_dbpedia_obj)+len(ga_translate_obj)} items (DBp: {len(ga_dbpedia_obj)}; GTr: {len(ga_translate_obj)}).')\n",
        "print(f'DBp en:\\t{len(list_dbp_en_obj)} {list_dbp_en_obj}')\n",
        "print(f'DBp {dest_language}:\\t{len(list_dbp_dest_obj)} {list_dbp_dest_obj}')\n",
        "print(f'GTr en:\\t{len(list_gtrans_en_obj)} {list_gtrans_en_obj}')\n",
        "print(f'GTr {dest_language}:\\t{len(list_gtrans_dest_obj)} {list_gtrans_dest_obj}')"
      ],
      "metadata": {
        "id": "k76ZPYmFnTgY",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build file with WebNLG EN and destination values\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "def build_tr_file(list_en, list_dest, dest_language, subj_obj, dbp_mt):\n",
        "  x = 0\n",
        "  fo = codecs.open(dest_language+'_'+subj_obj+'Values_'+dbp_mt+'Translations.txt', 'w', 'utf-8')\n",
        "  while x < len(list_en):\n",
        "    source_entity = list_en[x]\n",
        "    dest_entity_full = list_dest[x]\n",
        "    dest_entity_noParenth = re.sub('_\\(.*$', '', dest_entity_full)\n",
        "    # removing commas may not be a great idea, that's how we write addresses naturally in french (39, dargle road)\n",
        "    # dest_entity_noParenth_noComma = re.sub(',.*$', '', dest_entity_noParenth)\n",
        "    # Only write in file if values are actually different (this was added at a later stage, so the first vesion of the files have all pair regardless of the values)\n",
        "    if source_entity != dest_entity_full:\n",
        "      source_entity_final = []\n",
        "      dest_entity_final = []\n",
        "      separator = None\n",
        "      # If the entity ends with several commas, replace the last one by a coordination, and put that updated version with \"and\" in the list of entities to write\n",
        "      if re.search('[^,]+,[\\s_][^,]+,[\\s_][^,]+,[\\s_][^,]+$', source_entity) and not re.search('[^,]+,[\\s_][^,]+,[\\s_][^,]+[\\s_](and|or)[^,]+$', source_entity):\n",
        "        coord_source = (re.sub('([^,]+,[\\s_][^,]+,[\\s_][^,]+),([\\s_])([^,]+)$', '\\g<1>\\g<2>and\\g<2>\\g<3>', source_entity))\n",
        "        coord_dest = (re.sub('([^,]+,[\\s_][^,]+,[\\s_][^,]+),([\\s_])([^,]+)$', '\\g<1>\\g<2>et\\g<2>\\g<3>', dest_entity_full))\n",
        "        source_entity_final.append(coord_source)\n",
        "        dest_entity_final.append(coord_dest)\n",
        "        # If the entity has quotes, also add a non-quoted version of the coordinated entity\n",
        "        if re.search('^\"', source_entity):\n",
        "          source_entity_final.append(coord_source.replace('\"', ''))\n",
        "          dest_entity_final.append(coord_dest.replace('\"', ''))\n",
        "\n",
        "      # If the entity has quotes, put a version of it without quotes in the list of entities to write\n",
        "      if re.search('\"', source_entity):\n",
        "        source_entity_final.append(source_entity.replace('\"', ''))\n",
        "        dest_entity_final.append(dest_entity_full.replace('\"', ''))\n",
        "\n",
        "      # Also add all entities as such; just add the source as is and the destination with or without parentheses\n",
        "      source_entity_final.append(source_entity)\n",
        "      # If the original entity has a parenthesis or a comma, leave the parenthesis in the destination entity (simple check but wil work most of the time)\n",
        "      if re.search('\\(', source_entity) or re.search(',', source_entity):\n",
        "        dest_entity_final.append(dest_entity_full)\n",
        "      else:\n",
        "        dest_entity_final.append(dest_entity_noParenth)\n",
        "\n",
        "      # Stam's code had no tabs in the dbpedia files, so I keep doing the same to not break anything\n",
        "      if dbp_mt == 'dbpedia':\n",
        "        separator = '*'\n",
        "      else:\n",
        "        separator = '\\t*\\t'\n",
        "\n",
        "      y = 0\n",
        "      while y < len(source_entity_final):\n",
        "        source_entity_final_x = source_entity_final[y]\n",
        "        dest_entity_final_x = dest_entity_final[y]\n",
        "        # Write output file\n",
        "        fo.write(source_entity_final_x+separator+dest_entity_final_x+'\\n')\n",
        "        # For each input entity, write a lowercase and underscore counterpart, because lowercasing/underscoring is sometimes happening in the triple2predArg conversion\n",
        "        # I also found that some commas are replaced by \"and\"\n",
        "        if re.search('[A-Z]', source_entity_final_x) and re.search(' ', source_entity_final_x):\n",
        "          fo.write(source_entity_final_x.lower()+separator+dest_entity_final_x.lower()+'\\n')\n",
        "          fo.write(source_entity_final_x.replace(' ', '_')+separator+dest_entity_final_x+'\\n')\n",
        "          fo.write(source_entity_final_x.replace(' ', '_').lower()+separator+dest_entity_final_x.lower()+'\\n')\n",
        "        elif re.search('[A-Z]', source_entity_final_x):\n",
        "          fo.write(source_entity_final_x.lower()+separator+dest_entity_final_x.lower()+'\\n')\n",
        "        elif re.search(' ', source_entity_final_x):\n",
        "          fo.write(source_entity_final_x.replace(' ', '_')+separator+dest_entity_final_x+'\\n')\n",
        "        y += 1\n",
        "    x += 1\n",
        "  fo.close()\n",
        "\n",
        "# Create output files by pairing english and target language entities\n",
        "build_tr_file(list_dbp_en_subj, list_dbp_dest_subj, dest_language, 'sub', 'dbpedia')\n",
        "build_tr_file(list_dbp_en_obj, list_dbp_dest_obj, dest_language, 'obj', 'dbpedia')\n",
        "build_tr_file(list_gtrans_en_subj, list_gtrans_dest_subj, dest_language, 'sub', 'google')\n",
        "build_tr_file(list_gtrans_en_obj, list_gtrans_dest_obj, dest_language, 'obj', 'google')"
      ],
      "metadata": {
        "id": "MArtW3TRnTgY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WebNLG 2020 shared task data (system outputs, human ratings, etc.)"
      ],
      "metadata": {
        "id": "miTw8hM9h7_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/WebNLG/challenge-2020.git\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "evovKI-bh6oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print outputs of the different systems\n",
        "import glob\n",
        "import os\n",
        "import codecs\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# To wrap texts in cells\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "keepers = [5,6,20,23]\n",
        "\n",
        "# id = 24\n",
        "for id in keepers:\n",
        "  print(f'Text #{id}\\n----------\\n')\n",
        "  for input_triple in dataset_20_test[id][0]:\n",
        "    print(input_triple)\n",
        "  print('\\nReferences')\n",
        "  for reference in dataset_20_test[id][1]:\n",
        "    print(reference)\n",
        "  print('\\n')\n",
        "\n",
        "  for folder_path in sorted(glob.glob('/content/challenge-2020/submissions/rdf2text/en/*')):\n",
        "    # Get a list with all outputs of a system\n",
        "    outputs_file_lines = codecs.open(os.path.join(folder_path, 'primary.en'), 'r', 'utf-8').readlines()\n",
        "    # Get system name\n",
        "    system_name = folder_path.rsplit('/', 1)[1]\n",
        "    print(system_name)\n",
        "    print(outputs_file_lines[id])"
      ],
      "metadata": {
        "id": "5v4TZ_TwiQ2B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get system outputs according to human scores\n",
        "\n",
        "import json\n",
        "import codecs\n",
        "import statistics\n",
        "\n",
        "# The threshold t will be used to filter the texts that have a rating >= t when averaging all annotators for the selected criteria\n",
        "threshold = '95' #@param[100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 85, 80]\n",
        "threshold = int(threshold)\n",
        "\n",
        "FORGe_ratings = json.load(codecs.open('/content/challenge-2020/evaluation/human-evaluation/results/en/Baseline-FORGE2020/primary.json', 'r', 'utf-8'))\n",
        "Amazon_ratings = json.load(codecs.open('/content/challenge-2020/evaluation/human-evaluation/results/en/Amazon_AI_(Shanghai)/primary.json', 'r', 'utf-8'))\n",
        "OSU_ratings = json.load(codecs.open('/content/challenge-2020/evaluation/human-evaluation/results/en/OSU_Neural_NLG/primary.json', 'r', 'utf-8'))\n",
        "FB_ratings = json.load(codecs.open('/content/challenge-2020/evaluation/human-evaluation/results/en/FBConvAI/primary.json', 'r', 'utf-8'))\n",
        "\n",
        "Amazon_1textPerLine = codecs.open('/content/challenge-2020/submissions/rdf2text/en/Amazon_AI_(Shanghai)/primary.en', 'r', 'utf-8').readlines()\n",
        "OSU_1textPerLine = codecs.open('/content/challenge-2020/submissions/rdf2text/en/OSU_Neural_NLG/primary.en', 'r', 'utf-8').readlines()\n",
        "FB_1textPerLine = codecs.open('/content/challenge-2020/submissions/rdf2text/en/FBConvAI/primary.en', 'r', 'utf-8').readlines()\n",
        "\n",
        "\n",
        "def select_accurate_outputs(system_ratings, threshold):\n",
        "  kept_outputs = []\n",
        "  for system_output_id in system_ratings:\n",
        "    scores = []\n",
        "    ID_key = system_ratings[system_output_id]\n",
        "    for evaluator in ID_key:\n",
        "      # Select criteria to take into account\n",
        "      scores.append(ID_key[evaluator]['DataCoverage'])\n",
        "      scores.append(ID_key[evaluator]['Relevance'])\n",
        "      scores.append(ID_key[evaluator]['Correctness'])\n",
        "      # scores.append(ID_key[evaluator]['TextStructure'])\n",
        "      # scores.append(ID_key[evaluator]['Fluency'])\n",
        "    if len(scores) > 0:\n",
        "      if statistics.mean(scores) >= threshold:\n",
        "        kept_outputs.append(system_output_id)\n",
        "    else:\n",
        "      print(f'No scores for datapoint #{system_output_id}')\n",
        "    # print(scores)\n",
        "  print(f'{len(kept_outputs)}/{len(system_ratings)} outputs can be used!')\n",
        "  print(kept_outputs)\n",
        "  return kept_outputs\n",
        "\n",
        "def combine_lists(main_list, list_to_add):\n",
        "  for list_element in list_to_add:\n",
        "    if list_element not in main_list:\n",
        "      main_list.append(list_element)\n",
        "\n",
        "list_all_IDs_string = []\n",
        "# select_accurate_outputs returns a list of ID as strings; convert these to integers\n",
        "print('FORGe')\n",
        "list_FORGe_ids = [int(x) for x in select_accurate_outputs(FORGe_ratings, threshold)]\n",
        "print('AmazonAI')\n",
        "list_Amazon_ids = [int(x) for x in select_accurate_outputs(Amazon_ratings, threshold)]\n",
        "print('OSU')\n",
        "list_OSU_ids = [int(x) for x in select_accurate_outputs(OSU_ratings, threshold)]\n",
        "print('FBConvAI')\n",
        "list_FB_ids = [int(x) for x in select_accurate_outputs(FB_ratings, threshold)]\n",
        "\n",
        "# Get a list with all IDs for which we have at least one reference\n",
        "combine_lists(list_all_IDs_string, list_Amazon_ids)\n",
        "combine_lists(list_all_IDs_string, list_OSU_ids)\n",
        "combine_lists(list_all_IDs_string, list_FB_ids)\n",
        "\n",
        "print('All LLMs combined')\n",
        "# Sort the list to maintain alignment with the FORGe outputs later (I think)\n",
        "list_all_LLMout_IDs = sorted([int(x) for x in list_all_IDs_string])\n",
        "print(f'{len(list_all_LLMout_IDs)} IDs selected: {list_all_LLMout_IDs}.')\n",
        "\n",
        "# Now build a resource that looks like dataset_20_test (see \"Make data post-processing experiments\")\n",
        "# So we can run use the same functions as for the train/dev/test splits\n",
        "dataset_system_outputs = []\n",
        "for c, ID in enumerate(list_all_LLMout_IDs):\n",
        "  # For each datapoint, create two lists, the first one for the input (empty here), the second one for the texts\n",
        "  dataset_system_outputs.append([[], []])\n",
        "  if ID in list_Amazon_ids:\n",
        "    dataset_system_outputs[c][1].append(Amazon_1textPerLine[ID].strip())\n",
        "  if ID in list_OSU_ids:\n",
        "    if OSU_1textPerLine[ID].strip() not in dataset_system_outputs[c][1]:\n",
        "      dataset_system_outputs[c][1].append(OSU_1textPerLine[ID].strip())\n",
        "    # else:\n",
        "    #   print(f'Duplicate OSU {ID}')\n",
        "  if ID in list_FB_ids:\n",
        "    if FB_1textPerLine[ID].strip() not in dataset_system_outputs[c][1]:\n",
        "      dataset_system_outputs[c][1].append(FB_1textPerLine[ID].strip())\n",
        "    # else:\n",
        "    #   print(f'Duplicate FB {ID}')\n",
        "print(dataset_system_outputs)\n",
        "\n",
        "# list_Amazon_texts = [Amazon_1textPerLine[int(i)].strip() for i in list_Amazon_ids]\n",
        "# list_OSU_texts = [OSU_1textPerLine[int(i)].strip() for i in list_OSU_ids]\n",
        "# list_FB_texts = [FB_1textPerLine[int(i)].strip() for i in list_FB_ids]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DbUuchf5DvMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make fine-tuning data for post-processing experiments\n",
        "# The idea is to compile:\n",
        "# - all pairs <WebNLG-Triple>/<unique-ref-text> for train, dev, test\n",
        "# - all pairs <FORGe-output>/<unique-ref-text> for train, dev test\n",
        "# - all pairs <FORGe-output>/<accurate-and-fluent-LLM-output> for test\n",
        "# And the same 3 datasets as above but with multiple references for each input\n",
        "# We already stored the data quite simply: dataset_20_test[0][0] contains the triples of the first datapoint, dataset_20_test[0][1] contains the ref texts of the first datapoint\n",
        "# E.g. [['Ballistic_(comicsCharacter) | creator | Doug_Moench', 'Ballistic_(comicsCharacter) | creator | \"Michael Manley\"'], ['The creators of the comic character Ballistic were Michael Manley and Doug Moench.', 'Doug Moench and Michael Manley, created the comic character Ballistic.', 'The comic book character Ballistic was created by Michael Manley and Doug Moench.']]\n",
        "\n",
        "create_csv_triple2ref = 'no'#@param['yes', 'no']\n",
        "create_csv_forge2ref = 'no'#@param['yes', 'no']\n",
        "create_csv_forge2llm = 'yes'#@param['yes', 'no']\n",
        "\n",
        "txt_folder = '/content/textFiles'\n",
        "\n",
        "import csv\n",
        "import codecs\n",
        "import glob\n",
        "import os\n",
        "\n",
        "num_inputs_data20_train = len(dataset_20_train)\n",
        "num_inputs_data20_dev = len(dataset_20_dev)\n",
        "num_inputs_data20_test = 1779\n",
        "\n",
        "def check_num_datapoints_unique(count_original_inputs_test, pairs_input_test, count_original_inputs_dev, pairs_input_dev, count_original_inputs_train, pairs_input_train):\n",
        "  errors = 0\n",
        "  if (not count_original_inputs_test == 1779) or (not pairs_input_test == 5150):\n",
        "    print('  Error number test items!')\n",
        "    errors += 1\n",
        "  if (not count_original_inputs_dev == 1667) or (not pairs_input_dev == 4464):\n",
        "    print('  Error number dev items!')\n",
        "    errors += 1\n",
        "  if (not count_original_inputs_train == 13211) or (not pairs_input_train == 35426):\n",
        "    print('  Error number train items!')\n",
        "    errors += 1\n",
        "  if errors == 0:\n",
        "    print('  All good!')\n",
        "\n",
        "def check_num_datapoints_multi(count_original_inputs_test, pairs_input_test, count_original_inputs_dev, pairs_input_dev, count_original_inputs_train, pairs_input_train):\n",
        "  errors = 0\n",
        "  if (not count_original_inputs_test == 1779) or (not pairs_input_test == 1779):\n",
        "    print('  Error number test items!')\n",
        "    errors += 1\n",
        "  if (not count_original_inputs_dev == 1667) or (not pairs_input_dev == 1667):\n",
        "    print('  Error number dev items!')\n",
        "    errors += 1\n",
        "  if (not count_original_inputs_train == 13211) or (not pairs_input_train == 13211):\n",
        "    print('  Error number train items!')\n",
        "    errors += 1\n",
        "  if errors == 0:\n",
        "    print('  All good!')\n",
        "\n",
        "def create_csv(rows, dataset, split, reference_num, threshold = ''):\n",
        "  with open(f'WebNLG_{dataset}_{split}_{reference_num}{threshold}.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(rows)\n",
        "\n",
        "def write_text_file(list_texts, fileprefix, split):\n",
        "  with open(f'{fileprefix}_{split}.txt', 'w', encoding='UTF8') as fx:\n",
        "    for text in list_texts:\n",
        "      fx.write(text)\n",
        "      fx.write('\\n')\n",
        "\n",
        "def gather_ModD2T_texts_in_file(filepath, txt_folder):\n",
        "  \"\"\" To read a ModD2T file and store all texts in a list. \"\"\"\n",
        "  lines = codecs.open(filepath, 'r', 'utf-8').readlines()\n",
        "  texts_ModD2T = []\n",
        "  for line in lines:\n",
        "    if line.startswith('# text = '):\n",
        "      texts_ModD2T.append(re.subn('\\\\\\&', '&', line.split(' = ')[1])[0].strip())\n",
        "  # From the list, write the ModD2T FORGe outputs in a separate file to compare to the official WebNLG'20 submission\n",
        "  if filepath == '/content/Mod-D2T/conllu-en_INLG23/test/01-PredArgNorm.conllu':\n",
        "    write_text_file(texts_ModD2T, 'textFiles/WebNLG_ModD2T-out', 'test')\n",
        "  elif filepath == '/content/Mod-D2T/conllu-en_INLG23/dev/01-PredArgNorm.conllu':\n",
        "    write_text_file(texts_ModD2T, 'textFiles/WebNLG_ModD2T-out', 'dev')\n",
        "  elif filepath == '/content/Mod-D2T/conllu-en_INLG23/train/01-PredArgNorm.conllu':\n",
        "    write_text_file(texts_ModD2T, 'textFiles/WebNLG_ModD2T-out', 'train')\n",
        "\n",
        "def align_input_uniqueRefText(dataset, num_inputs, forge_texts = None):\n",
        "  \"\"\"\n",
        "  dataset = a list of the following: [['Ballistic_(comicsCharacter) | creator | Doug_Moench', 'Ballistic_(comicsCharacter) | creator | \"Michael Manley\"'], ['The creators of the comic character Ballistic were Michael Manley and Doug Moench.', 'Doug Moench and Michael Manley, created the comic character Ballistic.', 'The comic book character Ballistic was created by Michael Manley and Doug Moench.']]\n",
        "  num_inputs = an integer\n",
        "  To create the triple2ref data and the forge2ref data.\n",
        "  With this function, we aim at building alignments that can be exported into a CSV.\n",
        "  Each CSV row must be a list of 2 columns, and all rows must be kept in a list. Here's what one row should look like:\n",
        "  ['Nie_Haisheng | birthDate | 1964-10-13, Nie_Haisheng | occupation | Fighter_pilot', 'Nie Haisheng born on 10/13/1964 is a fighter pilot.']\n",
        "  \"\"\"\n",
        "  pairs = []\n",
        "  c = 0\n",
        "  count_texts = 0\n",
        "  # On HuggingFace, they concatenated all RDF2Text and Text2RDF test sets; the RDF2Text data with references consists of the first 1779 items (the next 1779 items are without refs, then it's Text2RDF)\n",
        "  # So we need to select only the first 1779 text lists of the test data (we use num_inputs to capture this number).\n",
        "  while c < num_inputs:\n",
        "    # For each data point, iterate over the individual reference texts\n",
        "    for text_id, reference_text in enumerate(dataset[c][1]):\n",
        "      # Now create the position in the pairs list, which is the current count_text counter + the current text_id\n",
        "      pairs_list_id = count_texts + text_id\n",
        "      # For each position, create a list and append the input and one reference text (the same input for all reference_texts of the same datapoint)\n",
        "      pairs.append([])\n",
        "      # If we are dealing with input triples, join the triples of each input into one string\n",
        "      if forge_texts == None:\n",
        "        pairs[pairs_list_id].append(', '.join(dataset[c][0]))\n",
        "      # Otherwise, just copy the text\n",
        "      else:\n",
        "         pairs[pairs_list_id].append(forge_texts[c])\n",
        "      pairs[pairs_list_id].append(reference_text)\n",
        "    # Update the count_texts counter with the last (highest) value of text_id, adding 1 because the text_id counter always starts at 0, so 0 is 1 text, 1 is 2, etc.\n",
        "    count_texts += text_id + 1\n",
        "    c += 1\n",
        "  return pairs, c\n",
        "\n",
        "def align_input_multipleRefTexts(dataset, num_inputs, forge_texts = None):\n",
        "  \"\"\" See comments in the function above for unique ref \"\"\"\n",
        "  pairs = []\n",
        "  c = 0\n",
        "  while c < num_inputs:\n",
        "    pairs.append([])\n",
        "    # If we are dealing with input triples, join the triples of each input into one string\n",
        "    if forge_texts == None:\n",
        "      pairs[c].append(', '.join(dataset[c][0]))\n",
        "    # Otherwise, just copy the FORGe text as input\n",
        "    else:\n",
        "        pairs[c].append(forge_texts[c])\n",
        "    # Add the reference texts\n",
        "    pairs[c].append(dataset[c][1])\n",
        "    # Update the count_texts counter with the last (highest) value of text_id, adding 1 because the text_id counter always starts at 0, so 0 is 1 text, 1 is 2, etc.\n",
        "    c += 1\n",
        "  return pairs, c\n",
        "\n",
        "if create_csv_triple2ref == 'yes':\n",
        "  print('Creating triple2ref data')\n",
        "  # Align inputs and outputs for unique ref\n",
        "  pairs_input_uniqueRef_test20, count_original_inputs_test20_u = align_input_uniqueRefText(dataset_20_test, num_inputs_data20_test)\n",
        "  pairs_input_uniqueRef_dev20, count_original_inputs_dev20_u = align_input_uniqueRefText(dataset_20_dev, num_inputs_data20_dev)\n",
        "  pairs_input_uniqueRef_train20, count_original_inputs_train20_u = align_input_uniqueRefText(dataset_20_train, num_inputs_data20_train)\n",
        "  # Align inputs and outputs for multiple refs\n",
        "  pairs_input_multiRefs_test20, count_original_inputs_test20_m = align_input_multipleRefTexts(dataset_20_test, num_inputs_data20_test)\n",
        "  pairs_input_multiRefs_dev20, count_original_inputs_dev20_m = align_input_multipleRefTexts(dataset_20_dev, num_inputs_data20_dev)\n",
        "  pairs_input_multiRefs_train20, count_original_inputs_train20_m = align_input_multipleRefTexts(dataset_20_train, num_inputs_data20_train)\n",
        "  # Create CSVs for unique ref\n",
        "  create_csv(pairs_input_uniqueRef_test20, 'triple2ref', 'test', 'uniqueRef')\n",
        "  create_csv(pairs_input_uniqueRef_dev20, 'triple2ref', 'dev', 'uniqueRef')\n",
        "  create_csv(pairs_input_uniqueRef_train20, 'triple2ref', 'train', 'uniqueRef')\n",
        "  # Create CSVs for multiple refs\n",
        "  create_csv(pairs_input_multiRefs_test20, 'triple2ref', 'test', 'multiRef')\n",
        "  create_csv(pairs_input_multiRefs_dev20, 'triple2ref', 'dev', 'multiRef')\n",
        "  create_csv(pairs_input_multiRefs_train20, 'triple2ref', 'train', 'multiRef')\n",
        "  # Check number of inputs\n",
        "  print('Checking triple2ref data unique ref')\n",
        "  check_num_datapoints_unique(count_original_inputs_test20_u, len(pairs_input_uniqueRef_test20), count_original_inputs_dev20_u, len(pairs_input_uniqueRef_dev20), count_original_inputs_train20_u, len(pairs_input_uniqueRef_train20))\n",
        "  print('Checking triple2ref data multiple refs')\n",
        "  check_num_datapoints_multi(count_original_inputs_test20_m, len(pairs_input_multiRefs_test20), count_original_inputs_dev20_m, len(pairs_input_multiRefs_dev20), count_original_inputs_train20_m, len(pairs_input_multiRefs_train20))\n",
        "\n",
        "if create_csv_forge2ref == 'yes':\n",
        "  print('Creating forge2ref data')\n",
        "  path_dev = '/content/Mod-D2T/conllu-en_INLG23/dev/01-PredArgNorm.conllu'\n",
        "  path_test = '/content/Mod-D2T/conllu-en_INLG23/test/01-PredArgNorm.conllu'\n",
        "  path_train = '/content/Mod-D2T/conllu-en_INLG23/train/01-PredArgNorm.conllu'\n",
        "  # Collect all FORGe texts into files\n",
        "  if not os.path.exists(txt_folder):\n",
        "    os.makedirs(txt_folder)\n",
        "  else:\n",
        "    files = glob.glob(txt_folder+'/*')\n",
        "    for f in files:\n",
        "      os.remove(f)\n",
        "  # Get latest texts for all splits from Mod-D2T dataset\n",
        "  gather_ModD2T_texts_in_file(path_test, txt_folder)\n",
        "  gather_ModD2T_texts_in_file(path_dev, txt_folder)\n",
        "  gather_ModD2T_texts_in_file(path_train, txt_folder)\n",
        "  # Once we have collected the texts, apply same postprocessing as regular FORGe outputs\n",
        "  print('Post processing of FORGe texts:')\n",
        "  ! python '/content/M-FleNS_NLG-Pipeline/code/postProcess.py' 'EN' {txt_folder}\n",
        "  # Finally, put the post_processed texts into a list to build our dataset\n",
        "  forge_test_texts = [text.strip() for text in codecs.open('textFiles/WebNLG_ModD2T-out_test_postproc.txt', 'r', 'utf-8').readlines()]\n",
        "  forge_dev_texts = [text.strip() for text in codecs.open('textFiles/WebNLG_ModD2T-out_dev_postproc.txt', 'r', 'utf-8').readlines()]\n",
        "  forge_train_texts = [text.strip() for text in codecs.open('textFiles/WebNLG_ModD2T-out_train_postproc.txt', 'r', 'utf-8').readlines()]\n",
        "  # Align inputs and outputs for unique ref\n",
        "  pairs_forge_uniqueRef_test20, count_original_inputs_test20_u = align_input_uniqueRefText(dataset_20_test, num_inputs_data20_test, forge_test_texts)\n",
        "  pairs_forge_uniqueRef_dev20, count_original_inputs_dev20_u = align_input_uniqueRefText(dataset_20_dev, num_inputs_data20_dev, forge_dev_texts)\n",
        "  pairs_forge_uniqueRef_train20, count_original_inputs_train20_u = align_input_uniqueRefText(dataset_20_train, num_inputs_data20_train, forge_train_texts)\n",
        "  # Align inputs and outputs for multiple refs\n",
        "  pairs_forge_multiRefs_test20, count_original_inputs_test20_m = align_input_multipleRefTexts(dataset_20_test, num_inputs_data20_test, forge_test_texts)\n",
        "  pairs_forge_multiRefs_dev20, count_original_inputs_dev20_m = align_input_multipleRefTexts(dataset_20_dev, num_inputs_data20_dev, forge_dev_texts)\n",
        "  pairs_forge_multiRefs_train20, count_original_inputs_train20_m = align_input_multipleRefTexts(dataset_20_train, num_inputs_data20_train, forge_train_texts)\n",
        "  # Create CSVs for unique ref\n",
        "  create_csv(pairs_forge_uniqueRef_test20, 'forge2ref', 'test', 'uniqueRef')\n",
        "  create_csv(pairs_forge_uniqueRef_dev20, 'forge2ref', 'dev', 'uniqueRef')\n",
        "  create_csv(pairs_forge_uniqueRef_train20, 'forge2ref', 'train', 'uniqueRef')\n",
        "  # Create CSVs for multiple refs\n",
        "  create_csv(pairs_forge_multiRefs_test20, 'forge2ref', 'test', 'multiRef')\n",
        "  create_csv(pairs_forge_multiRefs_dev20, 'forge2ref', 'dev', 'multiRef')\n",
        "  create_csv(pairs_forge_multiRefs_train20, 'forge2ref', 'train', 'multiRef')\n",
        "  # Check number of inputs\n",
        "  print('Checking forge2ref data unique ref')\n",
        "  check_num_datapoints_unique(count_original_inputs_test20_u, len(pairs_forge_uniqueRef_test20), count_original_inputs_dev20_u, len(pairs_forge_uniqueRef_dev20), count_original_inputs_train20_u, len(pairs_forge_uniqueRef_train20))\n",
        "  print('Checking forge2ref data muliple refs')\n",
        "  check_num_datapoints_multi(count_original_inputs_test20_m, len(pairs_forge_multiRefs_test20), count_original_inputs_dev20_m, len(pairs_forge_multiRefs_dev20), count_original_inputs_train20_m, len(pairs_forge_multiRefs_train20))\n",
        "\n",
        "if create_csv_forge2llm == 'yes':\n",
        "  path_test = '/content/Mod-D2T/conllu-en_INLG23/test/01-PredArgNorm.conllu'  # Collect all FORGe texts into files\n",
        "  if not os.path.exists(txt_folder):\n",
        "    os.makedirs(txt_folder)\n",
        "  else:\n",
        "    files = glob.glob(txt_folder+'/*')\n",
        "    for f in files:\n",
        "      os.remove(f)\n",
        "  # Get latest texts for test split from Mod-D2T dataset\n",
        "  gather_ModD2T_texts_in_file(path_test, txt_folder)\n",
        "  # Once we have collected the texts, apply same postprocessing as regular FORGe outputs\n",
        "  print('Post processing of FORGe texts:')\n",
        "  ! python '/content/M-FleNS_NLG-Pipeline/code/postProcess.py' 'EN' {txt_folder}\n",
        "  # Now select the text for which we have accurate LLM outputs (IDs saved in list_all_LLMout_IDs from the cell above)\n",
        "  forge_test_texts = [text.strip() for i, text in enumerate(codecs.open('textFiles/WebNLG_ModD2T-out_test_postproc.txt', 'r', 'utf-8').readlines()) if i in list_all_LLMout_IDs]\n",
        "  # forge_test_texts = []\n",
        "  # forge_1text_per_line = codecs.open('textFiles/WebNLG_ModD2T-out_test_postproc.txt', 'r', 'utf-8').readlines()\n",
        "  # for iforge, text in enumerate(forge_1text_per_line):\n",
        "  #   if iforge in list_all_LLMout_IDs:\n",
        "  #     print(iforge)\n",
        "  #     forge_test_texts.append(text)\n",
        "  # Align inputs and outputs for unique ref\n",
        "  pairs_forge_uniqueLLM_test20, count_original_inputs_test20_u = align_input_uniqueRefText(dataset_system_outputs, len(list_all_LLMout_IDs), forge_test_texts)\n",
        "  # Align inputs and outputs for multiple refs\n",
        "  pairs_forge_multiLLM_test20, count_original_inputs_test20_m = align_input_multipleRefTexts(dataset_system_outputs, len(list_all_LLMout_IDs), forge_test_texts)\n",
        "  # Create CSVs for unique ref\n",
        "  create_csv(pairs_forge_uniqueLLM_test20, 'forge2llm', 'test', 'uniqueRef', threshold)\n",
        "  # Create CSVs for multiple refs\n",
        "  create_csv(pairs_forge_multiLLM_test20, 'forge2llm', 'test', 'multiRef', threshold)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yTEmOhaDXmRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make test data for post-processing experiments\n",
        "import xmltodict\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "import codecs\n",
        "\n",
        "# Path to downloaded xml\n",
        "path_GEM_test = '/content/GEM24_test'\n",
        "path_GEM_forge = '/content/GEM24_forge'\n",
        "\n",
        "paths_GEM_input_files = glob.glob(os.path.join(path_GEM_test, '*.xml'))\n",
        "paths_GEM_FORGe_output_files = glob.glob(os.path.join(path_GEM_forge, '*.txt'))\n",
        "\n",
        "def create_csv(rows, dataset, inputType):\n",
        "  with open(f'{dataset}_{inputType}.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(rows)\n",
        "\n",
        "def create_test_data_GEM (input_file_path):\n",
        "  head, tail = os.path.split(input_file_path)\n",
        "  # Get input ytpe\n",
        "  input_type = ''\n",
        "  if tail.rsplit('.', 1)[1] == 'xml':\n",
        "    input_type = 'triples'\n",
        "  elif tail.rsplit('.', 1)[1] == 'txt':\n",
        "    input_type = 'forge'\n",
        "  dataset_id = ''\n",
        "\n",
        "  list_inputs = []\n",
        "  if input_type == 'triples':\n",
        "    # Get dataset name\n",
        "    dataset_id = tail.split('_', 1)[0]\n",
        "    input_xml_file = codecs.open(input_file_path, 'r', 'utf-8').read()\n",
        "    input_data_dict = xmltodict.parse(input_xml_file)\n",
        "    # Build a data structure like the one from HuggingFace for the other datasets\n",
        "    for entry in input_data_dict['benchmark']['entries']['entry']:\n",
        "      data_point = []\n",
        "      mtriples_list = []\n",
        "      # Get modified triples\n",
        "      if isinstance(entry['modifiedtripleset']['mtriple'], list):\n",
        "        for mtriple in entry['modifiedtripleset']['mtriple']:\n",
        "          mtriples_list.append(mtriple)\n",
        "      else:\n",
        "        mtriples_list.append(entry['modifiedtripleset']['mtriple'])\n",
        "      data_point.append(', '.join(mtriples_list))\n",
        "      list_inputs.append(data_point)\n",
        "  elif input_type == 'forge':\n",
        "    # Get dataset name\n",
        "    dataset_id = tail.split('_')[1]\n",
        "    forge_texts = codecs.open(input_file_path, 'r', 'utf-8').readlines()\n",
        "    for text in forge_texts:\n",
        "      list_inputs.append([text.strip()])\n",
        "\n",
        "  print(input_type, dataset_id)\n",
        "  create_csv(list_inputs, dataset_id, input_type)\n",
        "\n",
        "for file_path_i in paths_GEM_input_files:\n",
        "  create_test_data_GEM(file_path_i)\n",
        "\n",
        "for file_path_f in paths_GEM_FORGe_output_files:\n",
        "  create_test_data_GEM(file_path_f)"
      ],
      "metadata": {
        "id": "3Y97BEO5VAXb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "Ij9mghiPjjx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract predictions csv\n",
        "import csv\n",
        "import codecs\n",
        "import glob\n",
        "import os\n",
        "\n",
        "path_pred = '/content/drive/MyDrive/M-FleNS/Papers&Slides/M-FleNS_papers/2024-03_GEM-SharedTask/Test data/FORGe_T5Base_prediction/fulltune'\n",
        "path_sub = '/content/drive/MyDrive/M-FleNS/Papers&Slides/M-FleNS_papers/2024-03_GEM-SharedTask/Submissions/[FORGe]+[T5-Base(forge2text)]'\n",
        "files_pred_paths = glob.glob(os.path.join(path_pred, '*.csv'))\n",
        "\n",
        "for file_pred_path in files_pred_paths:\n",
        "  head, tail = os.path.split(file_pred_path)\n",
        "  filenameSub = 'FORGe-T5base_'+tail.rsplit('.', 1)[0].split('_')[0]+'_en.txt'\n",
        "  print(filenameSub)\n",
        "  with codecs.open(os.path.join(path_sub, filenameSub), 'w', 'utf-8') as fo:\n",
        "    with open(file_pred_path, newline='') as csvfile:\n",
        "      reader = csv.DictReader(csvfile)\n",
        "      row_current_count = 0\n",
        "      for row in reader:\n",
        "        fo.write(row['predictions'])\n",
        "        fo.write('\\n')"
      ],
      "metadata": {
        "id": "Llcbt_k9i3XV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}